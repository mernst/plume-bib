%% Bibliography for NLP (natural language processing), as applied to
%% software engineering tasks.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Classic NLP papers
%%%


@InProceedings{KleinM2002,
  author =       {Klein, Dan and Manning, Christopher D.},
  title =        {Fast exact inference with a factored model for natural language parsing},
  crossref =  "NIPS2002",
  pages =     {3-10},
  abstract =  {We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.},
  bibnote =   "The Stanford Parser",
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Criticisms of Tellina's accuracy
%%%

@InProceedings{NeurIPS-2020-NLC2CMD-Competition,
  title = 	 {{NeurIPS} 2020 {NLC2CMD} Competition: Translating Natural Language to {Bash} Commands},
  author =       {Agarwal, Mayank and Chakraborti, Tathagata and Fu, Quchen and Gros, David and Lin, Xi Victoria and Maene, Jaron and Talamadupula, Kartik and Teng, Zhongwei and White, Jules},
  booktitle = 	 {Proceedings of the NeurIPS 2020 Competition and Demonstration Track},
  pages = 	 {302--324},
  year = 	 {2021},
  volume = 	 {133},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 dec,
  pdf = 	 {http://proceedings.mlr.press/v133/agarwal21b/agarwal21b.pdf},
  url = 	 {https://proceedings.mlr.press/v133/agarwal21b.html},
  abstract = 	 {The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line. Participants were tasked with building models that can transform descriptions of command line tasks in English to their Bash syntax. This is a report on the competition with details of the task, metrics, data, attempted solutions, and lessons learned.}
}

@InProceedings{FuTWS2021,
  author = 	 "Fu, Quchen and Teng, Zhongwei and White, Jules and Schmidt, Douglas C.",
  title = 	 "A Transformer-based Approach for Translating Natural Language to {Bash} Commands",
  booktitle = "2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)",
  year = 	 2021,
  pages = 	 "1245-1248",
}


@InProceedings{ChenHLO2020,
  author = 	 "Chen, Yan and Herskovitz, Jaylin and Lasecki, Walter S. and Oney, Steve",
  title = 	 "Bashon: A Hybrid Crowd-Machine Workflow for Shell Command Synthesis",
  booktitle = "2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",
  year = 	 2020,
  pages = 	 "1-8",
  doi={10.1109/VL/HCC50065.2020.9127248}
}

@InProceedings{ZhangLXTZLZ2022,
  author = 	 "Neng Zhang and Chao Liu and Xin Xia and Christoph Treude and Ying Zou and David Lo and Zibin Zheng",
  title = 	 "{ShellFusion}: Answer Generation for Shell Programming Tasks via Knowledge Fusion",
  crossref =  "ICSE2022",
  NEEDpages = 	 "*",
}

@InProceedings{KanCW2020,
  author = 	 "Kan, Jia-Wei and Chien, Wei-Chin and Wang, Sheng-De",
  title = 	 "Grid Structure Attention for Natural Language Interface to {Bash} Commands",
  booktitle = "2020 International Computer Symposium (ICS)",
  year = 	 2020,
  pages = 	 "67-72",
  doi={10.1109/ICS51289.2020.00023},
}

@InProceedings{KumarNSAS2019,
  author = 	 "Kumar, NS and Nagalakshmi, Malathy and Sharma, Tanya and Ambati, Sai Bhavana and Satyanarayana, Vibha",
  title = 	 "Natural Language Interface to {Linux} Shell – Report",
  booktitle = "2019 3rd International Conference on Computing and Communications Technologies (ICCCT)",
  year = 	 2019,
  pages = 	 "24-30",
  doi={10.1109/ICCCT2.2019.8824800},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Participants in NLC2CMD competition
%%%


@inproceedings{VaswaniSPUJGKP2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {NeurIPS},
 LONGbooktitle = {Advances in Neural Information Processing Systems},
 NEEDpages = {},
 title = {Attention is All you Need},
 OMITurl = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{TangMRS2018,
  author = 	 "Gongbo Tang and Mathias M{\"{u}}ller and Annette Rios and Rico Sennrich",
  title = 	 "Why Self-Attention? {A} Targeted Evaluation of Neural Machine Translation Architectures",
  booktitle = "2018 Conference on Empirical Methods in Natural Language
Processing",
  year = 	 2018,
  address = 	 "Brussels, Belgium",
}

@Misc{Gros2019,
  author = 	 "David Gros",
  title = 	 "{AInix}: An Open Platform for Natural Language Interfaces to Shell Commands",
  month = 	 may,
  year = 	 2019,
  note = 	 "Undergraduate Honors Thesis, Computer Science Department, University of Texas at Austin",
  url="http://www.cs.utexas.edu/users/ai-labpub-view.php?PubID=127814",
}

@TechReport{RadfordWCLAS2019,
  author = 	 "Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever",
  title = 	 "Language models are unsupervised multitask learners",
  institution =  "OpenAI",
  year = 	 2019,
  url = "http://www.persagen.com/files/misc/radford2019language.pdf",
}

@TechReport{LinvinovMPKO2020,
  author = 	 "Denis Litvinov and Gleb Morgachev and Artem Popov and Nikolai Korolev and Dmitrii Orekhov",
  title = 	 "{NLC2CMD} Report from {JB} Team",
  institution =  "JetBrains",
  year = 	 2020,
  month = 	 dec,
  url = "https://github.com/JetBrains/nlc2cmd/blob/master/report.pdf",
}

@Misc{KangY2020,
  author = 	 "Sungmin Kang and Juyeon Yoon",
  title = 	 "Hierarchical Decoding of {Bash} Commands",
  year = 	 2020,
  note = 	 "Talk at NeurIPS 2020",
  url =          "https://slideslive.com/38942503/hierarchical-decoder-for-bash-commands",
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% NLP to command line (bash) tools
%%%

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 FULLbooktitle = {Advances in Neural Information Processing Systems (NeurIPS 2020)},
 booktitle = {NeurIPS 2020},
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 OMITurl = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@Article{LiWYT2019,
  author = 	 "Hao Li and Yu-Ping Wang and Jie Yin and Gang Tan",
  title = 	 "{SmartShell}: Automated Shell Scripts Synthesis from Natural Language",
  journal = 	 "International Journal of Software Engineering and Knowledge Engineering",
  year = 	 2019,
  volume = 	 29,
  number = 	 02,
  pages = 	 "197-220",
  doi = "https://doi.org/10.1142/S0218194019500098",
}

@InProceedings{CLAI-NeurIPS2019-demonstration,
  author = 	 "Mayank Agarwal and Jorge Barroso Carmona and Tathagata Chakraborti and Eli M. Dow and Kshitij P. Fadnis and Borja Godoy and Kartik Talamadupula",
  title = 	 "Project {CLAI} --- Bringing {AI} to the Command Line Interface",
  booktitle = "NeurIPS 2019 Demonstration Track",
  year = 	 2019,
}

@article{Agarwal2020ProjectCI,
  title={Project CLAI: Instrumenting the Command Line as a New Environment for AI Agents},
  author={Mayank Agarwal and Jorge J. Barroso and Tathagata Chakraborti and Eli M. Dow and Kshitij P. Fadnis and Borja Godoy and Madhavan Pallan and Kartik Talamadupula},
  journal={arXiv: Human-Computer Interaction},
  year={2020}
}

@Misc{CLAI-arxiv-2002.00762,
  author    = {Mayank Agarwal and
               Jorge J. Barroso and
               Tathagata Chakraborti and
               Eli M. Dow and
               Kshitij P. Fadnis and
               Borja Godoy and
               Kartik Talamadupula},
  title     = {{CLAI:} {A} Platform for {AI} Skills on the Command Line},
  howpublished = {https://arxiv.org/abs/2002.00762},
  url       = {https://arxiv.org/abs/2002.00762},
  month = 	 jun,
  year = 	 2020,
  note = 	 "v2",
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% NLP to test assertion tools
%%%


@InProceedings{DinellaRML2022,
  author = 	 "Dinella, Elizabeth and Ryan, Gabriel and Mytkowicz, Todd and Lahiri, Shuvendu K.",
  title = 	 "{TOGA}: a neural method for test oracle generation",
  crossref =  "ICSE2022",
  pages = 	 "2130-2141",
}


@INPROCEEDINGS {,
author = { Cheng, Xiang and Sang, Fan and Zhai, Yizhuo and Zhang, Xiaokuan and Kim, Taesoo },
booktitle = { 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE) },
title = {{ RUG: Turbo LLM for Rust Unit Test Generation }},
year = {2025},
volume = {},
ISSN = {1558-1225},
pages = {634-634},
abstract = { Unit testing improves software quality by evaluating isolated sections of the program. This approach alleviates the need for comprehensive program-wide testing and confines the potential error scope within the software. However, unit test development is time-consuming, requiring developers to create appropriate test contexts and determine input values to cover different code regions. This problem is particularly pronounced in Rust due to its intricate type system, making traditional unit test generation tools ineffective in Rust projects. Recently, LLM have demonstrated their proficiency in understanding programming language and completing software engineering tasks. However, merely prompting LLM with a basic prompt like 'generate unit test for the following source code' often results in code with compilation errors. In addition, LLM-generated unit tests often have limited test coverage. To bridge this gap and harness the capabilities of LLM, we design and implement RUG, an end-to-end solution to automatically generate the unit test for Rust projects. To help LLM's generated test pass Rust strict compilation checks, RUG designs a semantic-aware bottom-up approach to divide the context construction problem into dependent sub-problems. It solves these sub-problems sequentially using an LLM and merges them to a complete context. To increase test coverage, RUG integrates coverage-guided fuzzing with LLM to prepare fuzzing harnesses. Applying RUG on 17 real-world Rust programs (average 24,937 LoC), we show that RUG can achieve a high code coverage, up to 71.37\%, closely comparable to human effort (73.18\%). We submitted 113 unit tests generated by RUG covering the new code: 53 of them have been accepted, 17 rejected, and 43 are pending for review. },
keywords = {unit testing;large language model;rust},
doi = {10.1109/ICSE55347.2025.00097},
url = {https://doi.ieeecomputersociety.org/10.1109/ICSE55347.2025.00097},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}
This generates the whole test, not just the assertion.
Under the hood, it breaks the problem into parts.





@article{10.1145/3643769,
author = {Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi},
title = {Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using {LLM}},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
OMITurl = {https://doi.org/10.1145/3643769},
doi = {10.1145/3643769},
abstract = {Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage.
\par
In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26\% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {Large Language Models, Test Generation}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% NLP to (other) code tools
%%%






@INPROCEEDINGS{10852455,
  author={Michelutti, Chiara and Eckert, Jens and Monecke, Milko and Klein, Julian and Glesner, Sabine},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)},
  title={A Systematic Study on the Potentials and Limitations of LLM-assisted Software Development},
  year={2024},
  volume={},
  number={},
  pages={330-338},
  abstract={In the field of software engineering, Large Language Models like GPT have gained enormous interest in recent times. With its expanding area of application, ChatGPT has become an essential tool for code generation. Several studies have shown that the quality of generated code depends on the underlying dataset and the quality of the provided prompts. However, its precise capabilities and limitations remain uncertain, as does the extent of assistance required for effective code generation. We present the results of our systematic study in which we investigate the potential of ChatGPT, based on GPT-4, in solving assignments of an introductory-level programming class. We examine the impact of programming language choice, different prompting strategies, and the results of the model compared to those of real students. Our results show that ChatGPT cannot solve the assignments independently, but outperforms the average student with human assistance.},
  keywords={Computer languages;Java;Sequential analysis;Codes;Systematics;Large language models;Chatbots;Testing;Software engineering;Software development management;Large Language Models;Software Development;ChatGPT;Code Generation;Haskell;Java;Functional Programming;Object Oriented Programming;Prompt Engineering},
  doi={10.1109/FLLM63129.2024.10852455},
  ISSN={},
  month=Nov,}
I have not read this.
                  Abstract says it investigates "..., different prompting
                  strategies, ...".











@Misc{Jassy2024,
  author =       {Andy Jassy},
  title =        {Updating foundational software},
  howpublished = {\url{https://x.com/ajassy/status/1826608791741493281}},
  month =     aug,
  year =      2024,
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Code prompting advice
%%%








@inproceedings{10.1145/3545945.3569823,
author = {Denny, Paul and Kumar, Viraj and Giacaman, Nasser},
title = {Conversing with {Copilot}: Exploring Prompt Engineering for Solving {CS1} Problems Using Natural Language},
year = {2023},
isbn = {9781450394314},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3545945.3569823},
doi = {10.1145/3545945.3569823},
abstract = {GitHub Copilot is an artificial intelligence tool for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about its potential impact on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.},
booktitle = {SIGCSE},
pages = {1136–1142},
numpages = {7},
keywords = {artificial intelligence, cs1, foundation models, github copilot, introductory programming, large language models, openai},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}


@Inbook{White2024,
author="White, Jules
and Hays, Sam
and Fu, Quchen
and Spencer-Smith, Jesse
and Schmidt, Douglas C.",
editor="Nguyen-Duc, Anh
and Abrahamsson, Pekka
and Khomh, Foutse",
title="ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",
bookTitle="Generative AI for Effective Software Development",
year="2024",
OMITpublisher="Springer Nature Switzerland",
OMITaddress="Cham",
pages="71--108",
abstract="This chapter presents design techniques for software engineering, in the form of prompt patterns, to solve common problems that arise when using large language models (LLMs) to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and creating API specifications from lists of requirements. This chapter provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classifies patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, deployment, and testing.",
isbn="978-3-031-55642-5",
doi="10.1007/978-3-031-55642-5_4",
OMITurl="https://doi.org/10.1007/978-3-031-55642-5_4"
}

@article{10.1145/3672359.3672364,
author = {Schmidt, Douglas C. and Spencer-Smith, Jesse and Fu, Quchen and White, Jules},
title = {Towards a Catalog of Prompt Patterns to Enhance the Discipline of Prompt Engineering},
year = {2024},
issue_date = {December 2023},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1094-3641},
OMITurl = {https://doi.org/10.1145/3672359.3672364},
OMITdoi = {10.1145/3672359.3672364},
abstract = {The rapid advent of Large Language Models (LLMs), such as ChatGPT and Claude, is revolutionizing various fields, from education and healthcare to the engineering of reliable software systems. These LLMs operate through "prompts," which are natural language inputs that users employ to query and leverage the models' capabilities. Given the novelty of LLMs, the understanding of how to effectively use prompts remains largely anecdotal, based on isolated use cases. This fragmented approach limits the reliability and utility of LLMs, especially when they are applied in mission-critical software environments. To harness the full potential of LLMs in such crucial contexts, therefore, we need a systematic, disciplined approach to "prompt engineering" that guides interactions with and evaluations of these LLMs.},
journal = {Ada Lett.},
month = jun,
pages = {43–51},
numpages = {9}
}














@inproceedings{10.1145/3597503.3623316,
author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
title = {CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3597503.3623316},
doi = {10.1145/3597503.3623316},
abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70\% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code).To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation tasks carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation tasks from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the function under generation but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (CodeGen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating standalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {37},
numpages = {12},
keywords = {code generation, large language models, benchmark},
location = {Lisbon, Portugal},
series = {ICSE '24}
}













@INPROCEEDINGS{10298561,
  author={Ahmed, Toufique and Devanbu, Premkumar},
  booktitle={ASE},
  title={Better Patching Using {LLM} Prompting, via Self-Consistency},
  year={2023},
  volume={},
  number={},
  pages={1742-1746},
  abstract={Large Language models (LLMs) can be induced to solve non-trivial problems with “few-shot” prompts including illustrative problem-solution examples. Now if the few-shots also include “chain of thought” ($\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a “explained” solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\mathcal{S}-C$ (or even $\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.},
  keywords={Art;Maintenance engineering;Software;Cognition;Software engineering;LLMs;Self-consistency;Program Repair},
  doi={10.1109/ASE56229.2023.00065},
  ISSN={2643-1572},
  month=Sep,}
Advocates self-consistency, which is calling an LLM multiple times and choosing
                  the most common answer.






@article{10.1145/3675395,
author = {Li, Jia and Zhao, Yunfei and Li, Yongmin and Li, Ge and Jin, Zhi},
title = {{AceCoder}: An Effective Prompting Technique Specialized in Code Generation},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
OMITurl = {https://doi.org/10.1145/3675395},
doi = {10.1145/3675395},
abstract = {Large language models (LLMs) have shown great success in code generation. LLMs take as the input a prompt and output the code. How to make prompts (i.e., Prompting Techniques) is a key question. Existing prompting techniques are designed for natural language generation and have low accuracy in code generation.In this article, we propose a new prompting technique named AceCoder. Our motivation is that code generation meets two unique challenges (i.e., requirement understanding and code implementation). AceCoder contains two novel mechanisms (i.e., guided code generation and example retrieval) to solve these challenges. ❶ Guided code generation asks LLMs first to analyze requirements and output an intermediate preliminary (e.g., test cases). The preliminary clarifies requirements and tells LLMs “what to write.” ❷ Example retrieval selects similar programs as examples in prompts, which provide lots of relevant content (e.g., algorithms, APIs) and teach LLMs “how to write.” We apply AceCoder to four LLMs (e.g., GPT-3.5, CodeGeeX) and evaluate it on three public benchmarks using the Pass@ (k) . Results show that AceCoder can significantly improve the performance of LLMs on code generation. In terms of Pass@1, AceCoder outperforms the SOTA baseline by up to 56.4\% in MBPP, 70.7\% in MBJP, and 88.4\% in MBJSP. AceCoder is effective in LLMs with different sizes (i.e., 6B–13B) and different languages (i.e., Python, Java, and JavaScript). Human evaluation shows human developers prefer programs from AceCoder.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {204},
numpages = {26},
keywords = {Code generation, large language models, prompting engineering}
}


@article{10.1145/3724117,
author = {Huang, Dong and Zhang, Jie M. and Bu, Qingwen and Xie, Xiaofei and Chen, Junjie and Cui, Heming},
title = {Bias Testing and Mitigation in LLM-based Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
OMITurl = {https://doi.org/10.1145/3724117},
doi = {10.1145/3724117},
abstract = {As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47\% to 49.10\% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88\% to 4.79\% for GPT-4)1.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Fairness testing, code generation}
}





@inproceedings{10.1145/3634814.3634816,
author = {Cowan, Brendan and Watanobe, Yutaka and Shirafuji, Atsushi},
title = {Enhancing Programming Learning with LLMs: Prompt Engineering and Flipped Interaction},
year = {2024},
isbn = {9798400708534},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3634814.3634816},
doi = {10.1145/3634814.3634816},
abstract = {Due to their robustness, large language models (LLMs) are being utilized in many fields of study, including programming and education. Notably, they can be used by programmers by interfacing with their IDEs to assist with development, and in education by giving students meaningful and immediate feedback. In this paper, we propose and explore the groundwork of a framework designed to combine these two applications of LLMs. The framework acts as a facilitator between the LLM and the student by reading the student’s prompts before filtering and modifying them and sending them to the LLM. The intent is that this will improve the responses from the LLM, thereby improving the student’s learning experience. We discuss the framework in detail and analyze the value of individual responses returned from the LLM as a result of our framework. We conclude that the framework causes the LLM to give helpful responses in comparison to how it would respond without the framework.},
booktitle = {Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference},
pages = {10–16},
numpages = {7},
keywords = {ChatGPT, educational technology, large language models, programming education, prompt engineering},
location = {Aizu-Wakamatsu City, Japan},
series = {ASSE '23}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Code summarization
%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Models of code
%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LLMs
%%%

@inproceedings{10.1145/3491101.3519665,
author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
title = {Expectation vs.\ Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models},
year = {2022},
isbn = {9781450391566},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3491101.3519665},
doi = {10.1145/3491101.3519665},
abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants’ feedback.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {332},
numpages = {7},
keywords = {github copilot, large language model},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}






@InProceedings{ZamfirescuPereiraWHY2023,
  author =       {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
  title =        {Why {Johnny} Can’t Prompt: How Non-{AI} Experts Try (and Fail) to Design {LLM} Prompts},
  crossref =  {CHI2023},
  articleno =    437,
  doi =       {10.1145/3544548.3581388},
  abstract =  {Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.},
}


@InProceedings{DFISCAWRC2025,
  author =       {Ding, Yangruibo and Fu, Yanjun and Ibrahim, Omniyyah and Sitawarin, Chawin and Chen, Xinyun and Alomair, Basel and Wagner, David and Ray, Baishakhi and Chen, Yizheng},
  title =        {Vulnerability Detection with Code Language Models: How Far Are We?},
  crossref =  {ICSE2025},
  pages =     {469-481},
  abstract =
   {In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.
    To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.
    Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26\% F1 on BigVul but only 3.09\% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.},
}


@InProceedings{BarkeJP2023,
  author =       {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
  title =        {Grounded {Copilot}: How Programmers Interact with Code-Generating Models},
  crossref =  {OOPSLA2023},
  pages =     {78:1--78:27},
  abstract =  {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants—with a range of prior experience using the assistant—as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
}




@InProceedings{DeligiannisLMPR2025,
  author =       {Deligiannis, Pantazis and Lal, Akash and Mehrotra, Nikita and Poddar, Rishi and Rastogi, Aseem},
  title =        {{RustAssistant}: Using {LLMs} to Fix Compilation Errors in {Rust} Code},
  crossref =  {ICSE2025},
  pages =     {267-279},
  abstract =  {The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers. This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration between an LLM and the Rust compiler to deliver high accuracy of fixes. RustAssistant is able to achieve an impressive peak accuracy of roughly 74\% on real-world compilation errors in popular open-source Rust repositories. We also contribute a dataset of Rust compilation errors to enable further research.},
}




@article{10.1145/3660791,
author = {Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.},
title = {Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
OMITurl = {https://doi.org/10.1145/3660791},
doi = {10.1145/3660791},
abstract = {Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program’s intent. However, there is typically no guarantee that a program’s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The “emergent abilities” of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {84},
numpages = {24},
keywords = {Formal Specifications, Large Language Models, Postconditions}
}



@InProceedings{ShangCCZHYLZY2024,
  author =       {Xiuwei Shang and Shaoyin Cheng and Guoqiang Chen and Yanming Zhang and Li Hu and Xiao Yu and Gangyang Li and Weiming Zhang and Nenghai Yu},
  title =        {How Far Have We Gone in Binary Code Understanding Using Large Language Models},
  crossref =  {ICSME2024},
  NEEDpages =    {*},
}




@inproceedings{10.1145/3597503.3639187,
author = {Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
title = {Using an LLM to Help With Code Understanding},
year = {2024},
isbn = {9798400702174},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3597503.3639187},
doi = {10.1145/3597503.3639187},
abstract = {Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {97},
numpages = {13},
location = {Lisbon, Portugal},
series = {ICSE '24}
}



@inproceedings{10.1145/3658644.3690298,
author = {Nazzal, Mahmoud and Khalil, Issa and Khreishah, Abdallah and Phan, NhatHai},
title = {PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)},
year = {2024},
isbn = {9798400706363},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3658644.3690298},
doi = {10.1145/3658644.3690298},
abstract = {The capability of generating high-quality source code using large language models (LLMs) reduces software development time and costs. However, recent literature and our empirical investigation in this work show that while LLMs can generate functioning code, they inherently tend to introduce security vulnerabilities, limiting their potential. This problem is mainly due to their training on massive open-source corpora exhibiting insecure and inefficient programming practices. Therefore, automatic optimization of LLM prompts for generating secure and functioning code is a demanding need. This paper introduces PromSec, an algorithm for prompt optimization for secure and functioning code generation using LLMs. In PromSec, we combine 1) code vulnerability clearing using a generative adversarial graph neural network, dubbed as gGAN, to fix and reduce security vulnerabilities in generated codes and 2) code generation using an LLM into an interactive loop, such that the outcome of the gGAN drives the LLM with enhanced prompts to generate secure codes while preserving their functionality. Introducing a new contrastive learning approach in gGAN, we formulate the code-clearing and generation loop as a dual-objective optimization problem, enabling PromSec to notably reduce the number of LLM inferences. As a result, PromSec becomes a cost-effective and practical solution for generating secure and functioning codes.Extensive experiments conducted on Python and Java code datasets confirm that PromSec effectively enhances code security while upholding its intended functionality. Our experiments show that despite the comprehensive application of a state-of-the-art approach, it falls short in addressing all vulnerabilities within the code, whereas PromSec effectively resolves each of them. Moreover, PromSec achieves more than an order-of-magnitude reduction in operational time, number of LLM queries, and security analysis costs. Furthermore, prompts optimized with PromSec for a certain LLM are transferable to other LLMs across programming languages and generalizable to unseen vulnerabilities in training. This study presents an essential step towards improving the trustworthiness of LLMs for secure and functioning code generation, significantly enhancing their large-scale integration in real-world software code development practices.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2266–2280},
numpages = {15},
keywords = {LLMs, code generation, graph generative adversarial networks, secure and functioning codes},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}



@Article{app14031046,
AUTHOR = {Liu, Mingxing and Wang, Junfeng and Lin, Tao and Ma, Quan and Fang, Zhiyang and Wu, Yanqun},
TITLE = {An Empirical Study of the Code Generation of Safety-Critical Software Using {LLMs}},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {3},
ARTICLE-NUMBER = {1046},
URL = {https://www.mdpi.com/2076-3417/14/3/1046},
ISSN = {2076-3417},
ABSTRACT = {In the digital era of increasing software complexity, improving the development efficiency of safety-critical software is a challenging task faced by academia and industry in domains such as nuclear energy, aviation, the automotive industry, and rail transportation. Recently, people have been excited about using pre-trained large language models (LLMs) such as ChatGPT and GPT-4 to generate code. Professionals in the safety-critical software field are intrigued by the code generation capabilities of LLMs. However, there is currently a lack of systematic case studies in this area. Aiming at the need for automated code generation in safety-critical domains such as nuclear energy and the automotive industry, this paper conducts a case study on generating safety-critical software code using GPT-4 as the tool. Practical engineering cases from the industrial domain are employed. We explore different approaches, including code generation based on overall requirements, specific requirements, and augmented prompts. We propose a novel prompt engineering method called Prompt-FDC that integrates basic functional requirements, domain feature generalization, and domain constraints. This method improves code completeness from achieving 30% functions to 100% functions, increases the code comment rate to 26.3%, and yields better results in terms of code compliance, readability, and maintainability. The code generation approach based on LLMs also introduces a new software development process and V-model lifecycle for safety-critical software. Through systematic case studies, we demonstrate that, with appropriate prompt methods, LLMs can auto-generate safety-critical software code that meets practical engineering application requirements. It is foreseeable that LLMs can be applied to various engineering domains to improve software safety and development efficiency.},
DOI = {10.3390/app14031046}
}


@article{10.1145/3660810,
author = {Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing},
title = {{ClarifyGPT}: A Framework for Enhancing {LLM}-Based Code Generation via Requirements Clarification},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
OMITurl = {https://doi.org/10.1145/3660810},
doi = {10.1145/3660810},
abstract = {Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96\% to 80.80\% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43\% to 69.60\% and from 54.32\% to 62.37\%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {103},
numpages = {23},
keywords = {Code Generation, Large Language Model, Prompt Engineering}
}





@INPROCEEDINGS {MaLLXB2025,
author = { Ma, Lezhi and Liu, Shangqing and Li, Yi and Xie, Xiaofei and Bu, Lei },
booktitle = { 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE) },
title = {{ SpecGen: Automated Generation of Formal Program Specifications via Large Language Models }},
year = {2025},
volume = {},
ISSN = {1558-1225},
pages = {666-666},
abstract = { In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program. },
keywords = {program verification;specification inference;large language models},
doi = {10.1109/ICSE55347.2025.00129},
url = {https://doi.ieeecomputersociety.org/10.1109/ICSE55347.2025.00129},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}
  The technique consists of two parts:  "conversation-driven specification
generation", then "mutation-based specification generation".  The novelty lies
in the fix guidance and in mutating an incorrect specification to make it
correct.
  "conversation-driven specification generation" is not conversational in the
natural language sense.  It iterates with an LLM to produce a specification.  On
each iteration, ask the LLM for (an improved) specification; the input is the
code, the previous specification, the errors produced by the verifier for the
previous iteration, and guidance about how to fix the errors.  (This is rather
obvious -- It is used by RustAssistant [DeligiannisLMPR2025], for example --
except for the guidance.)  Terminate when the specification verifies
or after 10 rounds.  The prompt is few-shot:  it contains 4 randomly-chosen
examples from their dataset, together with their specifications.  I wonder how
similar different specifications are -- that is, how much the few-shot examples
help the LLM for this particular dataset.  For 4 of the executions (on
expectation), one of randomly-chosen examples will be exactly the program being
analyzed.
  For iterations after the first, the prompt gives guidance about correcting the
problem.  "through a massive amount of experiments, we summarize several types
of common verification failures reported by the verifier. For each kind of
error, we provide guidance in the natural language to facilitate the model in
generating correct specifications."  Does this imply that their technique is
difficult to apply to new situations?  They don't report what the guidance was.
They didn't evaluate the effect of the guidance (say, by an ablation study).
They don't report trying to do the correction symbolically.
  "Mutation-based specification generation" is really mutation-based
specification *correction*.  Even if the result of the previous step is not
correct, it may be close to correct.  Apply mutation operators to the
specification to see whether some variant of the specification verifies.  There
is some heuristic about the order in which to apply the mutations and
combinations of them.  This part does not use an LLM.  The 15 mutation operators
seem tuned to array invariants, which I suspect is the majority of their
invariants.  This correction is responsible for about 1/4 of SpecGen's overall
success.  AutoGen outperforms the "conversational specification generation";
if it were combined with the mutation-based correction, would that outperform
SpecGen?
  The paper gives only one example of output, in figure 1.
  I could not understand the explanation in section 3.C of the heuristics that
choose among mutated specifications.  Algorithm 1 contains no comments, and
there is such as "a subset E_selected of mutated specifications that can pass
the verification. The selected subset E_selected is initialized with E_t.", but
E_t consists of specifications that do *not* pass verification.  I think it may
work like this:  add up the cost of a set of mutations, and prioritize the
sets with the lowest cost.
  The evaluation is on a dataset of programs all of which verify perfectly with
OpenJML.  Any program that does not verify is dropped from their dataset and
does not appear in their experimental numbers.  The programs are selected from
the SV-COMP dataset (89\% of which are loop-free), from Nilizadeh et al. [43],
and from LeetCode.
  The "Prob." column in Table 2 and Table 6, is the "average success
probability" over 10 trials.  That is not as interesting as whether the
technique ever succeeds -- that is, prior work AutoSpec is just as good as their
SpecGen for the "Sequential" column, but the table only highlights their
SpecGen.
  It is no surprise that Daikon does poorly in their evaluation, since they
didn't subset their data to only programs whose specifications are within
Daikon's grammar.  Nor did they expand Daikon's grammar to include properties
that are necessary.  Nor do they report using Daikon to compute loop invariants
(which are necessary for verifying many of their programs).  The evaluation
isn't whether Daikon's output is correct or useful, only whether it forms a full
specification that OpenJML can fully verify (which is not a task Daikon is tuned
for).
  The paper is full of unfounded claims.  "The selected programs are highly
representative": no evidence is given.  "specifications that correctly and
comprehensively describe the semantics of complex programs": no evaluation of
the comprehensiveness is given.  We know that the generated specifications are
verifiable, but they might be quite weak and still verify.  Though OpenJML might
require that properties such as nullness and array indexing are satisfied.
  The authors did a user study on scoring how good the specifications were.
However, they did not include previous work, such as AutoSpec.  The raters were
the authors' graduate students, apparently.
  The artifact is at https://github.com/Lezhi-Ma/SpecGen-Artifact .


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Evaluation benchmarks
%%%



@inproceedings{10.5555/3618408.3619164,
author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
title = {DS-1000: a natural and reliable benchmark for data science code generation},
year = 2023,
publisher = {JMLR.org},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = 756,
numpages = 27,
location = {Honolulu, Hawaii, USA},
series = {ICML'23},
abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from Stack-Overflow. Second, our automatic evaluation is highly specific (reliable) - across all Codex-002- predicted solutions that our evaluation accepts, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3\% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
}




@INPROCEEDINGS {10174231,
author = { Tony, Catherine and Mutas, Markus and Ferreyra, Nicolas E. Diaz and Scandariato, Riccardo },
booktitle = { 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR) },
title = {{ LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations }},
year = {2023},
volume = {},
ISSN = {},
pages = {588-592},
abstract = { Large Language Models (LLMs) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language (NL) descriptions by learning languages and programming practices from public GitHub repositories. Although LLMs promise an effortless NL-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present LLMSecEval, a dataset containing 150 NL prompts that can be leveraged for assessing the security performance of such models. Such prompts are NL descriptions of code snippets prone to various security vulnerabilities listed in MITRE’s Top 25 Common Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by LLMs. As a practical application, we show how LLMSecEval can be used for evaluating the security of snippets automatically generated from NL descriptions. },
keywords = {Computer languages;Codes;Natural languages;Programming;Software;Security;Data mining},
doi = {10.1109/MSR59073.2023.00084},
url = {https://doi.ieeecomputersociety.org/10.1109/MSR59073.2023.00084},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}
The prompts are generated by Codex from CWE-related code snippets, then
"curated" (filtered? edited?) by the authors.
There are no test cases; the authors did an experiment where they ran their CodeQL
tool on the generated code:  "We used built-in QL queries to detect 18 of the
Top 25 CWEs in code created using LLMSecEval."










@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pond{\'e} and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mo Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  OMITurl={https://api.semanticscholar.org/CorpusID:235755472}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Other
%%%

@InProceedings{TanYKZ2007,
  author = 	 "Tan, Lin and Yuan, Ding and Krishna, Gopal and Zhou, Yuanyuan",
  title = 	 "/*{iComment}: Bugs or Bad Comments?*/",
  crossref =     "SOSP2007",
  pages = 	 "145--158",
}


@InProceedings{TanZP2011,
  author = 	 "Tan, Lin and Zhou, Yuanyuan and Padioleau, Yoann",
  title = 	 "{aComment}: Mining annotations from comments and code to detect interrupt related concurrency bugs",
  crossref =  "ICSE2011",
  pages = 	 "11--20",
}






@InProceedings{YeSMBL2016,
  author =       "Xin Ye and Hui Shen and Xiao Ma and Razvan Bunescu and Chang Liu",
  title =        "From word embeddings to document similarities for improved information retrieval in software engineering",
  crossref =     "ICSE2016",
  NEEDpages =     "*",
}
































@InProceedings{AllamanisBBS2014,
  author = 	 "Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles",
  title = 	 "Learning natural coding conventions",
  crossref =     "FSE2014",
  pages = 	 "281--293",
}


@InProceedings{PanditaXZXOP2012,
  author = 	 "Pandita, Rahul and Xiao, Xusheng and Zhong, Hao and Xie, Tao and Oney, Stephen and Paradkar, Amit",
  title = 	 "Inferring method specifications from natural language {API} descriptions",
  crossref =     "ICSE2012",
  pages = 	 "815--825",
}












@InProceedings{HindleBSGD2012,
  author = 	 "Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar",
  title = 	 "On the Naturalness of Software",
  crossref =     "ICSE2012",
  pages = 	 "837--847",
}




@InProceedings{HowardGPVS2013,
  author = 	 "Howard, Matthew J. and Gupta, Samir and Pollock, Lori and Vijay-Shanker, K.",
  title = 	 "Automatically mining software-based, semantically-similar words from comment-code mappings",
  crossref =     "MSR2013",
  pages = 	 "377--386",
}


@InProceedings{GuptaMPVS2013,
  author = 	 "Samir Gupta and Sana Malik and Lori Pollock and K. Vijay-Shanker",
  title = 	 "Part-of-speech tagging of program identifiers for improved text-based software engineering tools",
  crossref =     "ICPC2013",
  pages = 	 "3--12",
}


@InProceedings{SridharaHMPVS2010,
  author = 	 "Sridhara, Giriprasad and Hill, Emily and Muppaneni, Divya and Pollock, Lori and Vijay-Shanker, K.",
  title = 	 "Towards automatically generating summary comments for {Java} methods",
  crossref =     "ASE2010",
  pages = 	 "43--52",
}


@InProceedings{HillFBSNPV2008,
  author = 	 "Hill, Emily and Fry, Zachary P. and Boyd, Haley and Sridhara, Giriprasad and Novikova, Yana and Pollock, Lori and Vijay-Shanker, K.",
  title = 	 "{AMAP}: Automatically mining abbreviation expansions in programs to enhance software maintenance tools",
  crossref =     "MSR2008",
  pages = 	 "79--88",
}















































@InProceedings{ArnaoudovaEOGA2010,
  author = 	 "Arnaoudova, Venera and Eshkevari, Laleh and Oliveto, Rocco and Gueheneuc, Yann-Gael and Antoniol, Giuliano",
  title = 	 "Physical and conceptual identifier dispersion: Measures and relation to fault proneness",
  crossref =     "ICSM2010",
  pages = 	 "1--5",
}




@Article{LawrieMFB2007,
  author = 	 "Lawrie, Dawn and Morrell, Christopher and Feild, Henry and Binkley, David",
  title = 	 "Effective identifier names for comprehension and memory",
  journal = 	 "Innovations in Systems and Software Engineering",
  year = 	 2007,
  volume = 	 3,
  number = 	 4,
  pages = 	 "303--318",
  month = 	 dec,
  abstract =
   "Readers of programs have two main sources of domain information:
    identifier names and comments. When functions are uncommented, as many are,
    comprehension is almost exclusively dependent on the identifier
    names. Assuming that writers of programs want to create quality identifiers
    (e.g., identifiers that include relevant domain knowledge), one must ask
    how should they go about it. For example, do the initials of a concept name
    provide enough information to represent the concept? If not, and a longer
    identifier is needed, is an abbreviation satisfactory or does the concept
    need to be captured in an identifier that includes full words? What is the
    effect of longer identifiers on limited short term memory capacity? Results
    from a study designed to investigate these questions are reported. The
    study involved over 100 programmers who were asked to describe 12 different
    functions and then recall identifiers that appeared in each function. The
    functions used three different levels of identifiers: single letters,
    abbreviations, and full words. Responses allow the extent of comprehension
    associated with the different levels to be studied along with their impact
    on memory. The functions used in the study include standard computer
    science textbook algorithms and functions extracted from production
    code. The results show that full-word identifiers lead to the best
    comprehension; however, in many cases, there is no statistical difference
    between using full words and abbreviations. When considered in the light of
    limited human short-term memory, well-chosen abbreviations may be
    preferable in some situations since identifiers with fewer syllables are
    easier to remember.",
}


@Article{DeissenboeckP2006,
  author = 	 "Deissenboeck, Florian and Pizka, Markus",
  title = 	 "Concise and consistent naming",
  journal = 	 "Software Quality Journal",
  year = 	 2006,
  volume = 	 14,
  number = 	 3,
  pages = 	 "261--282",
  month = 	 sep,
}


@InProceedings{MihalceaCS2006,
  author = 	 "Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo",
  title = 	 "Corpus-based and knowledge-based measures of text semantic similarity",
  crossref =     "AAAI2006",
  pages = 	 "775--780",
}


@InProceedings{LawrieMB2010,
  author = 	 "Dawn Lawrie and Christopher Morrell and Dave Binkley",
  title = 	 "Normalizing source code vocabulary",
  crossref =     "WCRE2010",
  pages = 	 "3-12",
}




@InProceedings{MotwaniBrun2019,
  author = 	 "Motwani, Manish and Brun, Yuriy",
  title = 	 "Automatically Generating Precise Oracles from Structured Natural Language Specifications",
  crossref =  "ICSE2019",
  pages = 	 "188--199",
}


@InProceedings{HuLXLJ2018,
  author = 	 "Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi",
  title = 	 "Deep code comment generation",
  crossref =  "ICPC2018",
  pages = 	 "200--210",
}


@Misc{LouisDBS2018,
  author = 	 "Annie Louis and Santanu Kumar Dash and Earl T. Barr and Charles Sutton",
  title = 	 "Deep Learning to Detect Redundant Method Comments",
  howpublished = "\url{http://arxiv.org/abs/1806.04616}",
  month = 	 jun,
  year = 	 2018,
}


@InProceedings{MikolovSCCD2013,
  author = 	 "Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey",
  title = 	 "Distributed representations of words and phrases and their compositionality",
  crossref =  "NIPS2013",
  pages = 	 "3111--3119",
}

@InProceedings{MovshovitzAttiasC2013,
  author = 	 "Movshovitz-Attias, Dana and Cohen, William W.",
  title = 	 "Natural language models for predicting programming comments",
  crossref =  "ACL2013short",
  pages = 	 "35--40",
}

@InProceedings{BuzeW2010,
  author = 	 "Buse, Raymond P.L. and Weimer, Westley R.",
  title = 	 "Automatically documenting program changes",
  crossref =  "ASE2010",
  pages = 	 "33-42",
}

@InProceedings{PascarellaB2017,
  author = 	 "Pascarella, Luca and Bacchelli, Alberto",
  title = 	 "Classifying code comments in {Java} open-source software systems",
  crossref =  "MSR2017",
  pages = 	 "227-237",
  supersededby = "PascarellaBB2019"
}

@Article{PascarellaBB2019,
  author = 	 "Pascarella, Luca and Bruntink, Magiel and Bacchelli, Alberto",
  title = 	 "Classifying code comments in {Java} software systems",
  journal = 	 JEmpiricalSE,
  year = 	 2019,
  volume = 	 24,
  number = 	 3,
  pages = 	 "1499-1537",
  month = 	 jun,
}



@InProceedings{DevlinCLT2019,
  author = 	 "Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova",
  title = 	 "{BERT}: Pre-training of deep bidirectional transformers for language understanding",
  crossref =  "NAACL-HLT2019",
  pages = 	 "4171--4186",
}


@InProceedings{HossainD2025,
  author =       {Hossain, Soneya Binta and Dwyer, Matthew},
  title =        {{TOGLL}: Correct and Strong Test Oracle Generation with {LLMs}},
  crossref =  {ICSE2025},
  abstract =  {Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have shown impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation. In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on a large dataset consisting of 110 Java projects. Utilizing the most effective fine-tuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 unseen large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles than TOGA. Regarding bug detection effectiveness, TOGLL can detect 1,023 unique mutants that EvoSuite cannot, which is ten times more than what TOGA can detect. Additionally, TOGLL significantly outperforms TOGA in detecting real bugs from the Defects4J dataset.},
}


@INPROCEEDINGS{10189162,
  author={Siddiq, Mohammed Latif and Samee, Abdus and Azgor, Sk Ruhul and Haider, Md. Asif and Sawraz, Shehabul Islam and Santos, Joanna C. S.},
  booktitle={2023 IEEE/ACM 2nd International Workshop on Natural Language-Based Software Engineering (NLBSE)},
  title={Zero-shot Prompting for Code Complexity Prediction Using GitHub Copilot},
  year={2023},
  volume={},
  number={},
  pages={56-59},
  abstract={Code generation models are gaining popularity because they can produce correct code from a prompt, speeding up the software development process. GitHub Copilot is currently one of the most commonly used tools for code generation. This tool is based on GPT3, a Large Language Model (LLM), and can perform zero-shot prompting tasks i.e., tasks for which the model is not specifically trained. In this paper, we describe a preliminary study that investigates whether GitHub Copilot can predict the runtime complexity of a given program using zero- shot prompting. In our study, we found that GitHub Copilot can correctly predict the runtime complexity 45.44% times in the first suggestion and 56.38 % times considering all suggestions. We also compared Copilot to other machine learning, neural network, and transformer-based approaches for code complexity prediction. We observed that Copilot outperformed other approaches for predicting code with linear complexity $\mathbf{O}(n)$.},
  keywords={Codes;Runtime;Computational modeling;Predictive models;Transformers;Software;Complexity theory;code generation;computational complexity;trans-former;zero-shot prompting;pre-trained model;GitHub copilot},
  doi={10.1109/NLBSE59153.2023.00018},
  ISSN={},
  month=May,}
GitHub Copilot can correctly predict the runtime complexity 45.44\% times in the
first suggestion (top-1) and 56.38 \% times considering all suggestions (top-all).
Short 4-page paper.  They don't come out and say that one should re-prompt.










%  LocalWords:  InProceedings TanYKZ2007 Gopal Zhou Yuanyuan iComment Iyer pdf
%  LocalWords:  booktitle SOSP2007 SOSP2007date SOSP2007addr Benwen NN pre url
%  LocalWords:  testEntrySetClearChangesMap Srinivasan Ioannis Konstas Xin
%  LocalWords:  testSettingHeightThatIsTooSmallLeavesHeightUnchanged LSTM
%  LocalWords:  Zettlemoyer YeSMBL2016 Shen Xiao Razvan Bunescu Liu MRR Za
%  LocalWords:  ICSE2016 NEEDpages ICSE2016date ICSE2016addr Wiki LSA CCG
%  LocalWords:  stemmer Kushman Barzilay Turkers regex regexes Mise Kiddon
%  LocalWords:  Ganesa Thandavam Ponnuraj Yejin Choi Branavan Miltiadis xj
%  LocalWords:  Allamanis AAAI Briand Briand's Hirschberg uncompelling xk
%  LocalWords:  Movshovitz Attias ICPC preprocess pickaxe xl Convolutional
%  LocalWords:  Hao Peng ie camelcase tokenizer Naturalize's NLC2CMD OpenAI
% LocalWords:  Agarwal Mayank Chakraborti Tathagata Fu Quchen Gros
% LocalWords:  Maene Jaron Talamadupula Kartik Teng Zhongwei
