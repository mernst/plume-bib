% Bibliography for NLP (natural language processing), as applied to
%% software engineering tasks.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Classic NLP papers
%%%


@InProceedings{KleinM2002,
  author =       {Klein, Dan and Manning, Christopher D.},
  title =        {Fast exact inference with a factored model for natural language parsing},
  crossref =  "NIPS2002",
  pages =     {3-10},
  abstract =  {We present a novel generative model for natural language tree structures in which semantic (lexical dependency) and syntactic (PCFG) structures are scored with separate models. This factorization provides conceptual simplicity, straightforward opportunities for separately improving the component models, and a level of performance comparable to similar, non-factored models. Most importantly, unlike other modern parsing models, the factored model admits an extremely effective A* parsing algorithm, which enables efﬁcient, exact inference.},
  bibnote =   "The Stanford Parser",
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Criticisms of Tellina's accuracy
%%%

@InProceedings{NeurIPS-2020-NLC2CMD-Competition,
  title = 	 {{NeurIPS} 2020 {NLC2CMD} Competition: Translating Natural Language to {Bash} Commands},
  author =       {Agarwal, Mayank and Chakraborti, Tathagata and Fu, Quchen and Gros, David and Lin, Xi Victoria and Maene, Jaron and Talamadupula, Kartik and Teng, Zhongwei and White, Jules},
  booktitle = 	 {Proceedings of the NeurIPS 2020 Competition and Demonstration Track},
  pages = 	 {302--324},
  year = 	 {2021},
  volume = 	 {133},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 dec,
  pdf = 	 {http://proceedings.mlr.press/v133/agarwal21b/agarwal21b.pdf},
  url = 	 {https://proceedings.mlr.press/v133/agarwal21b.html},
  abstract = 	 {The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line. Participants were tasked with building models that can transform descriptions of command line tasks in English to their Bash syntax. This is a report on the competition with details of the task, metrics, data, attempted solutions, and lessons learned.}
}

@InProceedings{FuTWS2021,
  author = 	 "Fu, Quchen and Teng, Zhongwei and White, Jules and Schmidt, Douglas C.",
  title = 	 "A Transformer-based Approach for Translating Natural Language to {Bash} Commands",
  booktitle = "2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)",
  year = 	 2021,
  pages = 	 "1245-1248",
}


@InProceedings{ChenHLO2020,
  author = 	 "Chen, Yan and Herskovitz, Jaylin and Lasecki, Walter S. and Oney, Steve",
  title = 	 "Bashon: A Hybrid Crowd-Machine Workflow for Shell Command Synthesis",
  booktitle = "2020 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)",
  year = 	 2020,
  pages = 	 "1-8",
  doi={10.1109/VL/HCC50065.2020.9127248}
}

@InProceedings{ZhangLXTZLZ2022,
  author = 	 "Neng Zhang and Chao Liu and Xin Xia and Christoph Treude and Ying Zou and David Lo and Zibin Zheng",
  title = 	 "{ShellFusion}: Answer Generation for Shell Programming Tasks via Knowledge Fusion",
  crossref =  "ICSE2022",
  NEEDpages = 	 "*",
}

@InProceedings{KanCW2020,
  author = 	 "Kan, Jia-Wei and Chien, Wei-Chin and Wang, Sheng-De",
  title = 	 "Grid Structure Attention for Natural Language Interface to {Bash} Commands",
  booktitle = "2020 International Computer Symposium (ICS)",
  year = 	 2020,
  pages = 	 "67-72",
  doi={10.1109/ICS51289.2020.00023},
}

@InProceedings{KumarNSAS2019,
  author = 	 "Kumar, NS and Nagalakshmi, Malathy and Sharma, Tanya and Ambati, Sai Bhavana and Satyanarayana, Vibha",
  title = 	 "Natural Language Interface to {Linux} Shell – Report",
  booktitle = "2019 3rd International Conference on Computing and Communications Technologies (ICCCT)",
  year = 	 2019,
  pages = 	 "24-30",
  doi={10.1109/ICCCT2.2019.8824800},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Participants in NLC2CMD competition
%%%


@inproceedings{VaswaniSPUJGKP2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {NeurIPS},
 LONGbooktitle = {Advances in Neural Information Processing Systems},
 NEEDpages = {},
 title = {Attention is All you Need},
 OMITurl = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{TangMRS2018,
  author = 	 "Gongbo Tang and Mathias M{\"{u}}ller and Annette Rios and Rico Sennrich",
  title = 	 "Why Self-Attention? {A} Targeted Evaluation of Neural Machine Translation Architectures",
  booktitle = "2018 Conference on Empirical Methods in Natural Language
Processing",
  year = 	 2018,
  address = 	 "Brussels, Belgium",
}

@Misc{Gros2019,
  author = 	 "David Gros",
  title = 	 "{AInix}: An Open Platform for Natural Language Interfaces to Shell Commands",
  month = 	 may,
  year = 	 2019,
  note = 	 "Undergraduate Honors Thesis, Computer Science Department, University of Texas at Austin",
  url="http://www.cs.utexas.edu/users/ai-labpub-view.php?PubID=127814",
}

@TechReport{RadfordWCLAS2019,
  author = 	 "Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever",
  title = 	 "Language models are unsupervised multitask learners",
  institution =  "OpenAI",
  year = 	 2019,
  url = "http://www.persagen.com/files/misc/radford2019language.pdf",
}

@TechReport{LinvinovMPKO2020,
  author = 	 "Denis Litvinov and Gleb Morgachev and Artem Popov and Nikolai Korolev and Dmitrii Orekhov",
  title = 	 "{NLC2CMD} Report from {JB} Team",
  institution =  "JetBrains",
  year = 	 2020,
  month = 	 dec,
  url = "https://github.com/JetBrains/nlc2cmd/blob/master/report.pdf",
}

@Misc{KangY2020,
  author = 	 "Sungmin Kang and Juyeon Yoon",
  title = 	 "Hierarchical Decoding of {Bash} Commands",
  year = 	 2020,
  note = 	 "Talk at NeurIPS 2020",
  url =          "https://slideslive.com/38942503/hierarchical-decoder-for-bash-commands",
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% NLP to command line (bash) tools
%%%

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 FULLbooktitle = {Advances in Neural Information Processing Systems (NeurIPS 2020)},
 booktitle = {NeurIPS 2020},
 pages = {1877--1901},
 title = {Language Models are Few-Shot Learners},
 OMITurl = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@Article{LiWYT2019,
  author = 	 "Hao Li and Yu-Ping Wang and Jie Yin and Gang Tan",
  title = 	 "{SmartShell}: Automated Shell Scripts Synthesis from Natural Language",
  journal = 	 "International Journal of Software Engineering and Knowledge Engineering",
  year = 	 2019,
  volume = 	 29,
  number = 	 02,
  pages = 	 "197-220",
  doi = "https://doi.org/10.1142/S0218194019500098",
}

@InProceedings{CLAI-NeurIPS2019-demonstration,
  author = 	 "Mayank Agarwal and Jorge Barroso Carmona and Tathagata Chakraborti and Eli M. Dow and Kshitij P. Fadnis and Borja Godoy and Kartik Talamadupula",
  title = 	 "Project {CLAI} --- Bringing {AI} to the Command Line Interface",
  booktitle = "NeurIPS 2019 Demonstration Track",
  year = 	 2019,
}

@article{Agarwal2020ProjectCI,
  title={Project CLAI: Instrumenting the Command Line as a New Environment for AI Agents},
  author={Mayank Agarwal and Jorge J. Barroso and Tathagata Chakraborti and Eli M. Dow and Kshitij P. Fadnis and Borja Godoy and Madhavan Pallan and Kartik Talamadupula},
  journal={arXiv: Human-Computer Interaction},
  year={2020}
}

@Misc{CLAI-arxiv-2002.00762,
  author    = {Mayank Agarwal and
               Jorge J. Barroso and
               Tathagata Chakraborti and
               Eli M. Dow and
               Kshitij P. Fadnis and
               Borja Godoy and
               Kartik Talamadupula},
  title     = {{CLAI:} {A} Platform for {AI} Skills on the Command Line},
  howpublished = {https://arxiv.org/abs/2002.00762},
  url       = {https://arxiv.org/abs/2002.00762},
  month = 	 jun,
  year = 	 2020,
  note = 	 "v2",
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% NLP to test assertion tools
%%%


@InProceedings{DinellaRML2022,
  author = 	 "Dinella, Elizabeth and Ryan, Gabriel and Mytkowicz, Todd and Lahiri, Shuvendu K.",
  title = 	 "{TOGA}: a neural method for test oracle generation",
  crossref =  "ICSE2022",
  pages = 	 "2130-2141",
}


@INPROCEEDINGS {,
author = { Cheng, Xiang and Sang, Fan and Zhai, Yizhuo and Zhang, Xiaokuan and Kim, Taesoo },
booktitle = { 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE) },
title = {{ RUG: Turbo LLM for Rust Unit Test Generation }},
year = {2025},
volume = {},
ISSN = {1558-1225},
pages = {634-634},
abstract = { Unit testing improves software quality by evaluating isolated sections of the program. This approach alleviates the need for comprehensive program-wide testing and confines the potential error scope within the software. However, unit test development is time-consuming, requiring developers to create appropriate test contexts and determine input values to cover different code regions. This problem is particularly pronounced in Rust due to its intricate type system, making traditional unit test generation tools ineffective in Rust projects. Recently, LLM have demonstrated their proficiency in understanding programming language and completing software engineering tasks. However, merely prompting LLM with a basic prompt like 'generate unit test for the following source code' often results in code with compilation errors. In addition, LLM-generated unit tests often have limited test coverage. To bridge this gap and harness the capabilities of LLM, we design and implement RUG, an end-to-end solution to automatically generate the unit test for Rust projects. To help LLM's generated test pass Rust strict compilation checks, RUG designs a semantic-aware bottom-up approach to divide the context construction problem into dependent sub-problems. It solves these sub-problems sequentially using an LLM and merges them to a complete context. To increase test coverage, RUG integrates coverage-guided fuzzing with LLM to prepare fuzzing harnesses. Applying RUG on 17 real-world Rust programs (average 24,937 LoC), we show that RUG can achieve a high code coverage, up to 71.37\%, closely comparable to human effort (73.18\%). We submitted 113 unit tests generated by RUG covering the new code: 53 of them have been accepted, 17 rejected, and 43 are pending for review. },
keywords = {unit testing;large language model;rust},
doi = {10.1109/ICSE55347.2025.00097},
url = {https://doi.ieeecomputersociety.org/10.1109/ICSE55347.2025.00097},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}
This generates the whole test, not just the assertion.
Under the hood, it breaks the problem into parts.





@article{10.1145/3643769,
author = {Ryan, Gabriel and Jain, Siddhartha and Shang, Mingyue and Wang, Shiqi and Ma, Xiaofei and Ramanathan, Murali Krishna and Ray, Baishakhi},
title = {Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using {LLM}},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
OMITurl = {https://doi.org/10.1145/3643769},
doi = {10.1145/3643769},
abstract = {Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage.
\par
In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26\% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {Large Language Models, Test Generation}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% NLP to (other) code tools
%%%






@INPROCEEDINGS{10852455,
  author={Michelutti, Chiara and Eckert, Jens and Monecke, Milko and Klein, Julian and Glesner, Sabine},
  booktitle={2024 2nd International Conference on Foundation and Large Language Models (FLLM)},
  title={A Systematic Study on the Potentials and Limitations of LLM-assisted Software Development},
  year={2024},
  volume={},
  number={},
  pages={330-338},
  abstract={In the field of software engineering, Large Language Models like GPT have gained enormous interest in recent times. With its expanding area of application, ChatGPT has become an essential tool for code generation. Several studies have shown that the quality of generated code depends on the underlying dataset and the quality of the provided prompts. However, its precise capabilities and limitations remain uncertain, as does the extent of assistance required for effective code generation. We present the results of our systematic study in which we investigate the potential of ChatGPT, based on GPT-4, in solving assignments of an introductory-level programming class. We examine the impact of programming language choice, different prompting strategies, and the results of the model compared to those of real students. Our results show that ChatGPT cannot solve the assignments independently, but outperforms the average student with human assistance.},
  keywords={Computer languages;Java;Sequential analysis;Codes;Systematics;Large language models;Chatbots;Testing;Software engineering;Software development management;Large Language Models;Software Development;ChatGPT;Code Generation;Haskell;Java;Functional Programming;Object Oriented Programming;Prompt Engineering},
  doi={10.1109/FLLM63129.2024.10852455},
  ISSN={},
  month=Nov,}
I have not read this.
                  Abstract says it investigates "..., different prompting
                  strategies, ...".











@Misc{Jassy2024,
  author =       {Andy Jassy},
  title =        {Updating foundational software},
  howpublished = {\url{https://x.com/ajassy/status/1826608791741493281}},
  month =     aug,
  year =      2024,
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Code prompting advice
%%%








@inproceedings{10.1145/3545945.3569823,
author = {Denny, Paul and Kumar, Viraj and Giacaman, Nasser},
title = {Conversing with {Copilot}: Exploring Prompt Engineering for Solving {CS1} Problems Using Natural Language},
year = {2023},
isbn = {9781450394314},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3545945.3569823},
doi = {10.1145/3545945.3569823},
abstract = {GitHub Copilot is an artificial intelligence tool for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about its potential impact on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60\% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.},
booktitle = {SIGCSE},
pages = {1136–1142},
numpages = {7},
keywords = {artificial intelligence, cs1, foundation models, github copilot, introductory programming, large language models, openai},
location = {Toronto ON, Canada},
}


@Inbook{White2024,
author="White, Jules
and Hays, Sam
and Fu, Quchen
and Spencer-Smith, Jesse
and Schmidt, Douglas C.",
editor="Nguyen-Duc, Anh
and Abrahamsson, Pekka
and Khomh, Foutse",
title="ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",
bookTitle="Generative AI for Effective Software Development",
year="2024",
OMITpublisher="Springer Nature Switzerland",
OMITaddress="Cham",
pages="71--108",
abstract="This chapter presents design techniques for software engineering, in the form of prompt patterns, to solve common problems that arise when using large language models (LLMs) to automate common software engineering activities, such as ensuring code is decoupled from third-party libraries and creating API specifications from lists of requirements. This chapter provides two contributions to research on using LLMs for software engineering. First, it provides a catalog of patterns for software engineering that classifies patterns according to the types of problems they solve. Second, it explores several prompt patterns that have been applied to improve requirements elicitation, rapid prototyping, code quality, deployment, and testing.",
isbn="978-3-031-55642-5",
doi="10.1007/978-3-031-55642-5_4",
OMITurl="https://doi.org/10.1007/978-3-031-55642-5_4"
}

@article{10.1145/3672359.3672364,
author = {Schmidt, Douglas C. and Spencer-Smith, Jesse and Fu, Quchen and White, Jules},
title = {Towards a Catalog of Prompt Patterns to Enhance the Discipline of Prompt Engineering},
year = {2024},
issue_date = {December 2023},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1094-3641},
OMITurl = {https://doi.org/10.1145/3672359.3672364},
OMITdoi = {10.1145/3672359.3672364},
abstract = {The rapid advent of Large Language Models (LLMs), such as ChatGPT and Claude, is revolutionizing various fields, from education and healthcare to the engineering of reliable software systems. These LLMs operate through "prompts," which are natural language inputs that users employ to query and leverage the models' capabilities. Given the novelty of LLMs, the understanding of how to effectively use prompts remains largely anecdotal, based on isolated use cases. This fragmented approach limits the reliability and utility of LLMs, especially when they are applied in mission-critical software environments. To harness the full potential of LLMs in such crucial contexts, therefore, we need a systematic, disciplined approach to "prompt engineering" that guides interactions with and evaluations of these LLMs.},
journal = {Ada Lett.},
month = jun,
pages = {43–51},
numpages = {9}
}














@inproceedings{10.1145/3597503.3623316,
author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
title = {CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3597503.3623316},
doi = {10.1145/3597503.3623316},
abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70\% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code).To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation tasks carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation tasks from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the function under generation but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (CodeGen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating standalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.},
booktitle = {ICSE: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {37},
numpages = {12},
keywords = {code generation, large language models, benchmark},
location = {Lisbon, Portugal},
}













@INPROCEEDINGS{10298561,
  author={Ahmed, Toufique and Devanbu, Premkumar},
  booktitle={ASE},
  title={Better Patching Using {LLM} Prompting, via Self-Consistency},
  year={2023},
  volume={},
  number={},
  pages={1742-1746},
  abstract={Large Language models (LLMs) can be induced to solve non-trivial problems with “few-shot” prompts including illustrative problem-solution examples. Now if the few-shots also include “chain of thought” ($\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a “explained” solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\mathcal{S}-C$ (or even $\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.},
  keywords={Art;Maintenance engineering;Software;Cognition;Software engineering;LLMs;Self-consistency;Program Repair},
  doi={10.1109/ASE56229.2023.00065},
  ISSN={2643-1572},
  month=Sep,}
Advocates self-consistency, which is calling an LLM multiple times and choosing
                  the most common answer.






@article{10.1145/3675395,
author = {Li, Jia and Zhao, Yunfei and Li, Yongmin and Li, Ge and Jin, Zhi},
title = {{AceCoder}: An Effective Prompting Technique Specialized in Code Generation},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {8},
issn = {1049-331X},
OMITurl = {https://doi.org/10.1145/3675395},
doi = {10.1145/3675395},
abstract = {Large language models (LLMs) have shown great success in code generation. LLMs take as the input a prompt and output the code. How to make prompts (i.e., Prompting Techniques) is a key question. Existing prompting techniques are designed for natural language generation and have low accuracy in code generation.In this article, we propose a new prompting technique named AceCoder. Our motivation is that code generation meets two unique challenges (i.e., requirement understanding and code implementation). AceCoder contains two novel mechanisms (i.e., guided code generation and example retrieval) to solve these challenges. ❶ Guided code generation asks LLMs first to analyze requirements and output an intermediate preliminary (e.g., test cases). The preliminary clarifies requirements and tells LLMs “what to write.” ❷ Example retrieval selects similar programs as examples in prompts, which provide lots of relevant content (e.g., algorithms, APIs) and teach LLMs “how to write.” We apply AceCoder to four LLMs (e.g., GPT-3.5, CodeGeeX) and evaluate it on three public benchmarks using the Pass@ (k) . Results show that AceCoder can significantly improve the performance of LLMs on code generation. In terms of Pass@1, AceCoder outperforms the SOTA baseline by up to 56.4\% in MBPP, 70.7\% in MBJP, and 88.4\% in MBJSP. AceCoder is effective in LLMs with different sizes (i.e., 6B–13B) and different languages (i.e., Python, Java, and JavaScript). Human evaluation shows human developers prefer programs from AceCoder.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {204},
numpages = {26},
keywords = {Code generation, large language models, prompting engineering}
}


@article{10.1145/3724117,
author = {Huang, Dong and Zhang, Jie M. and Bu, Qingwen and Xie, Xiaofei and Chen, Junjie and Cui, Heming},
title = {Bias Testing and Mitigation in LLM-based Code Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
OMITurl = {https://doi.org/10.1145/3724117},
doi = {10.1145/3724117},
abstract = {As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47\% to 49.10\% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88\% to 4.79\% for GPT-4)1.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
keywords = {Fairness testing, code generation}
}





@inproceedings{10.1145/3634814.3634816,
author = {Cowan, Brendan and Watanobe, Yutaka and Shirafuji, Atsushi},
title = {Enhancing Programming Learning with LLMs: Prompt Engineering and Flipped Interaction},
year = {2024},
isbn = {9798400708534},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3634814.3634816},
doi = {10.1145/3634814.3634816},
abstract = {Due to their robustness, large language models (LLMs) are being utilized in many fields of study, including programming and education. Notably, they can be used by programmers by interfacing with their IDEs to assist with development, and in education by giving students meaningful and immediate feedback. In this paper, we propose and explore the groundwork of a framework designed to combine these two applications of LLMs. The framework acts as a facilitator between the LLM and the student by reading the student’s prompts before filtering and modifying them and sending them to the LLM. The intent is that this will improve the responses from the LLM, thereby improving the student’s learning experience. We discuss the framework in detail and analyze the value of individual responses returned from the LLM as a result of our framework. We conclude that the framework causes the LLM to give helpful responses in comparison to how it would respond without the framework.},
booktitle = {ASSE: Proceedings of the 2023 4th Asia Service Sciences and Software Engineering Conference},
pages = {10–16},
numpages = {7},
keywords = {ChatGPT, educational technology, large language models, programming education, prompt engineering},
location = {Aizu-Wakamatsu City, Japan},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Prompt engineering (not for software development tasks)
%%%

@article{10.1145/3731756,
author = {Ma, Qianou and Peng, Weirui and Yang, Chenyang and Shen, Hua and Koedinger, Ken and Wu, Tongshuang},
title = {What Should We Engineer in Prompts? Training Humans in Requirement-Driven LLM Use},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3731756},
doi = {10.1145/3731756},
abstract = {Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot) needs humans to clearly articulate customized requirements (e.g., “start the response with a tl;dr”). However, existing prompt engineering instructions often lack focused training on requirement articulation and instead tend to emphasize increasingly automatable strategies (e.g., tricks like adding role-plays and “think step-by-step”). To address the gap, we introduce Requirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human attention on generating clear, complete requirements during prompting. We implement ROPE through an assessment and training suite that provides deliberate practice with LLM-generated feedback. In a randomized controlled experiment with 30 novices, ROPE significantly outperforms conventional prompt engineering training (20\% vs. 1\% gains), a gap that automatic prompt optimization cannot close. Furthermore, we demonstrate a direct correlation between the quality of input requirements and LLM outputs. Our work paves the way to empower more end-users to build complex LLM applications.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = aug,
articleno = {41},
numpages = {27},
keywords = {LLM, Human-AI Interaction, Prompt Engineering, Requirement Engineering, End-User Programming}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Code summarization
%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Models of code
%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LLMs
%%%

@inproceedings{10.1145/3491101.3519665,
author = {Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L.},
title = {Expectation vs.\ Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models},
year = {2022},
isbn = {9781450391566},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3491101.3519665},
doi = {10.1145/3491101.3519665},
abstract = {Recent advances in Large Language Models (LLM) have made automatic code generation possible for real-world programming tasks in general-purpose programming languages such as Python. However, there are few human studies on the usability of these tools and how they fit the programming workflow. In this work, we conducted a within-subjects user study with 24 participants to understand how programmers use and perceive Copilot, a LLM-based code generation tool. We found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online. However, participants did face difficulties in understanding, editing, and debugging code snippets generated by Copilot, which significantly hindered their task-solving effectiveness. Finally, we highlighted several promising directions for improving the design of Copilot based on our observations and participants’ feedback.},
booktitle = {CHI EA: Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {332},
numpages = {7},
keywords = {github copilot, large language model},
location = {New Orleans, LA, USA},
}






@InProceedings{ZamfirescuPereiraWHY2023,
  author =       {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
  title =        {Why {Johnny} Can’t Prompt: How Non-{AI} Experts Try (and Fail) to Design {LLM} Prompts},
  crossref =  {CHI2023},
  articleno =    437,
  doi =       {10.1145/3544548.3581388},
  abstract =  {Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.},
}


@InProceedings{DFISCAWRC2025,
  author =       {Ding, Yangruibo and Fu, Yanjun and Ibrahim, Omniyyah and Sitawarin, Chawin and Chen, Xinyun and Alomair, Basel and Wagner, David and Ray, Baishakhi and Chen, Yizheng},
  title =        {Vulnerability Detection with Code Language Models: How Far Are We?},
  crossref =  {ICSE2025},
  pages =     {469-481},
  abstract =
   {In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.
    To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.
    Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26\% F1 on BigVul but only 3.09\% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.},
}


@InProceedings{BarkeJP2023,
  author =       {Barke, Shraddha and James, Michael B. and Polikarpova, Nadia},
  title =        {Grounded {Copilot}: How Programmers Interact with Code-Generating Models},
  crossref =  {OOPSLA2023},
  pages =     {78:1--78:27},
  abstract =  {Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But what is this new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants—with a range of prior experience using the assistant—as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants are bimodal: in acceleration mode, the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants.},
}




@InProceedings{DeligiannisLMPR2025,
  author =       {Deligiannis, Pantazis and Lal, Akash and Mehrotra, Nikita and Poddar, Rishi and Rastogi, Aseem},
  title =        {{RustAssistant}: Using {LLMs} to Fix Compilation Errors in {Rust} Code},
  crossref =  {ICSE2025},
  pages =     {267-279},
  abstract =  {The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers. This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration between an LLM and the Rust compiler to deliver high accuracy of fixes. RustAssistant is able to achieve an impressive peak accuracy of roughly 74\% on real-world compilation errors in popular open-source Rust repositories. We also contribute a dataset of Rust compilation errors to enable further research.},
}




@article{10.1145/3660791,
author = {Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K.},
title = {Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
OMITurl = {https://doi.org/10.1145/3660791},
doi = {10.1145/3660791},
abstract = {Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program’s intent. However, there is typically no guarantee that a program’s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language, which makes natural language intent challenging to check programmatically. The “emergent abilities” of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice.     In this paper, we describe nl2postcondition, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions.   We introduce and validate metrics to measure and compare different nl2postcondition approaches, using the correctness and discriminative power of generated postconditions.   We then use qualitative and quantitative methods to assess the quality of nl2postcondition postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that  via LLMs has the potential to be helpful in practice;  generated postconditions were able to catch 64 real-world historical bugs from Defects4J.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {84},
numpages = {24},
keywords = {Formal Specifications, Large Language Models, Postconditions}
}



@InProceedings{ShangCCZHYLZY2024,
  author =       {Xiuwei Shang and Shaoyin Cheng and Guoqiang Chen and Yanming Zhang and Li Hu and Xiao Yu and Gangyang Li and Weiming Zhang and Nenghai Yu},
  title =        {How Far Have We Gone in Binary Code Understanding Using Large Language Models},
  crossref =  {ICSME2024},
  pages =    {1-12},
}




@inproceedings{10.1145/3597503.3639187,
author = {Nam, Daye and Macvean, Andrew and Hellendoorn, Vincent and Vasilescu, Bogdan and Myers, Brad},
title = {Using an LLM to Help With Code Understanding},
year = {2024},
isbn = {9798400702174},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3597503.3639187},
doi = {10.1145/3597503.3639187},
abstract = {Understanding code is challenging, especially when working in new and complex development environments. Code comments and documentation can help, but are typically scarce or hard to navigate. Large language models (LLMs) are revolutionizing the process of writing code. Can they do the same for helping understand it? In this study, we provide a first investigation of an LLM-based conversational UI built directly in the IDE that is geared towards code understanding. Our IDE plugin queries OpenAI's GPT-3.5-turbo model with four high-level requests without the user having to write explicit prompts: to explain a highlighted section of code, provide details of API calls used in the code, explain key domain-specific terms, and provide usage examples for an API. The plugin also allows for open-ended prompts, which are automatically contextualized to the LLM with the program being edited. We evaluate this system in a user study with 32 participants, which confirms that using our plugin can aid task completion more than web search. We additionally provide a thorough analysis of the ways developers use, and perceive the usefulness of, our system, among others finding that the usage and benefits differ between students and professionals. We conclude that in-IDE prompt-less interaction with LLMs is a promising future direction for tool builders.},
booktitle = {ICSE: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {97},
numpages = {13},
location = {Lisbon, Portugal},
}



@inproceedings{10.1145/3658644.3690298,
author = {Nazzal, Mahmoud and Khalil, Issa and Khreishah, Abdallah and Phan, NhatHai},
title = {PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs)},
year = {2024},
isbn = {9798400706363},
OMITpublisher = {Association for Computing Machinery},
OMITaddress = {New York, NY, USA},
OMITurl = {https://doi.org/10.1145/3658644.3690298},
doi = {10.1145/3658644.3690298},
abstract = {The capability of generating high-quality source code using large language models (LLMs) reduces software development time and costs. However, recent literature and our empirical investigation in this work show that while LLMs can generate functioning code, they inherently tend to introduce security vulnerabilities, limiting their potential. This problem is mainly due to their training on massive open-source corpora exhibiting insecure and inefficient programming practices. Therefore, automatic optimization of LLM prompts for generating secure and functioning code is a demanding need. This paper introduces PromSec, an algorithm for prompt optimization for secure and functioning code generation using LLMs. In PromSec, we combine 1) code vulnerability clearing using a generative adversarial graph neural network, dubbed as gGAN, to fix and reduce security vulnerabilities in generated codes and 2) code generation using an LLM into an interactive loop, such that the outcome of the gGAN drives the LLM with enhanced prompts to generate secure codes while preserving their functionality. Introducing a new contrastive learning approach in gGAN, we formulate the code-clearing and generation loop as a dual-objective optimization problem, enabling PromSec to notably reduce the number of LLM inferences. As a result, PromSec becomes a cost-effective and practical solution for generating secure and functioning codes.Extensive experiments conducted on Python and Java code datasets confirm that PromSec effectively enhances code security while upholding its intended functionality. Our experiments show that despite the comprehensive application of a state-of-the-art approach, it falls short in addressing all vulnerabilities within the code, whereas PromSec effectively resolves each of them. Moreover, PromSec achieves more than an order-of-magnitude reduction in operational time, number of LLM queries, and security analysis costs. Furthermore, prompts optimized with PromSec for a certain LLM are transferable to other LLMs across programming languages and generalizable to unseen vulnerabilities in training. This study presents an essential step towards improving the trustworthiness of LLMs for secure and functioning code generation, significantly enhancing their large-scale integration in real-world software code development practices.},
booktitle = {CCS: Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2266–2280},
numpages = {15},
keywords = {LLMs, code generation, graph generative adversarial networks, secure and functioning codes},
location = {Salt Lake City, UT, USA},
}



@Article{app14031046,
AUTHOR = {Liu, Mingxing and Wang, Junfeng and Lin, Tao and Ma, Quan and Fang, Zhiyang and Wu, Yanqun},
TITLE = {An Empirical Study of the Code Generation of Safety-Critical Software Using {LLMs}},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {3},
ARTICLE-NUMBER = {1046},
URL = {https://www.mdpi.com/2076-3417/14/3/1046},
ISSN = {2076-3417},
ABSTRACT = {In the digital era of increasing software complexity, improving the development efficiency of safety-critical software is a challenging task faced by academia and industry in domains such as nuclear energy, aviation, the automotive industry, and rail transportation. Recently, people have been excited about using pre-trained large language models (LLMs) such as ChatGPT and GPT-4 to generate code. Professionals in the safety-critical software field are intrigued by the code generation capabilities of LLMs. However, there is currently a lack of systematic case studies in this area. Aiming at the need for automated code generation in safety-critical domains such as nuclear energy and the automotive industry, this paper conducts a case study on generating safety-critical software code using GPT-4 as the tool. Practical engineering cases from the industrial domain are employed. We explore different approaches, including code generation based on overall requirements, specific requirements, and augmented prompts. We propose a novel prompt engineering method called Prompt-FDC that integrates basic functional requirements, domain feature generalization, and domain constraints. This method improves code completeness from achieving 30% functions to 100% functions, increases the code comment rate to 26.3%, and yields better results in terms of code compliance, readability, and maintainability. The code generation approach based on LLMs also introduces a new software development process and V-model lifecycle for safety-critical software. Through systematic case studies, we demonstrate that, with appropriate prompt methods, LLMs can auto-generate safety-critical software code that meets practical engineering application requirements. It is foreseeable that LLMs can be applied to various engineering domains to improve software safety and development efficiency.},
DOI = {10.3390/app14031046}
}


@article{10.1145/3660810,
author = {Mu, Fangwen and Shi, Lin and Wang, Song and Yu, Zhuohao and Zhang, Binquan and Wang, ChenXue and Liu, Shichao and Wang, Qing},
title = {{ClarifyGPT}: A Framework for Enhancing {LLM}-Based Code Generation via Requirements Clarification},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
OMITurl = {https://doi.org/10.1145/3660810},
doi = {10.1145/3660810},
abstract = {Large Language Models (LLMs), such as ChatGPT, have demonstrated impressive capabilities in automatically generating code from provided natural language requirements. However, in real-world practice, it is inevitable that the requirements written by users might be ambiguous or insufficient. Current LLMs will directly generate programs according to those unclear requirements, regardless of interactive clarification, which will likely deviate from the original user intents. To bridge that gap, we introduce a novel framework named ClarifyGPT, which aims to enhance code generation by empowering LLMs with the ability to identify ambiguous requirements and ask targeted clarifying questions. Specifically, ClarifyGPT first detects whether a given requirement is ambiguous by performing a code consistency check. If it is ambiguous, ClarifyGPT prompts an LLM to generate targeted clarifying questions. After receiving question responses, ClarifyGPT refines the ambiguous requirement and inputs it into the same LLM to generate a final code solution. To evaluate our ClarifyGPT, we invite ten participants to use ClarifyGPT for code generation on two benchmarks: MBPP-sanitized and MBPP-ET. The results show that ClarifyGPT elevates the performance (Pass@1) of GPT-4 from 70.96\% to 80.80\% on MBPP-sanitized. Furthermore, to conduct large-scale automated evaluations of ClarifyGPT across different LLMs and benchmarks without requiring user participation, we introduce a high-fidelity simulation method to simulate user responses. The results demonstrate that ClarifyGPT can significantly enhance code generation performance compared to the baselines. In particular, ClarifyGPT improves the average performance of GPT-4 and ChatGPT across five benchmarks from 62.43\% to 69.60\% and from 54.32\% to 62.37\%, respectively. A human evaluation also confirms the effectiveness of ClarifyGPT in detecting ambiguous requirements and generating high-quality clarifying questions. We believe that ClarifyGPT can effectively facilitate the practical application of LLMs in real-world development environments.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {103},
numpages = {23},
keywords = {Code Generation, Large Language Model, Prompt Engineering}
}





@INPROCEEDINGS {MaLLXB2025,
author = { Ma, Lezhi and Liu, Shangqing and Li, Yi and Xie, Xiaofei and Bu, Lei },
booktitle = { 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE) },
title = {{ SpecGen: Automated Generation of Formal Program Specifications via Large Language Models }},
year = {2025},
volume = {},
ISSN = {1558-1225},
pages = {666-666},
abstract = { In the software development process, formal program specifications play a crucial role in various stages, including requirement analysis, software testing, and verification. However, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. Moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. To reduce the burden on software developers, automated specification generation methods have emerged. However, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. To tackle this challenge, we introduce SpecGen, a novel technique for formal program specification generation based on Large Language Models (LLMs). Our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of LLMs. The process of SpecGen consists of two phases. The first phase employs a conversational approach that guides the LLM in generating appropriate specifications for a given program, aiming to utilize the ability of LLM to generate high-quality specifications. The second phase, designed for where the LLM fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. We evaluate SpecGen on two datasets, including the SV-COMP Java category benchmark and a manually constructed dataset containing 120 programs. Experimental results demonstrate that SpecGen succeeds in generating verifiable specifications for 279 out of 385 programs, outperforming the existing LLM-based approaches and conventional specification generation tools like Houdini and Daikon. Further investigations on the quality of generated specifications indicate that SpecGen can comprehensively articulate the behaviors of the input program. },
keywords = {program verification;specification inference;large language models},
doi = {10.1109/ICSE55347.2025.00129},
url = {https://doi.ieeecomputersociety.org/10.1109/ICSE55347.2025.00129},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}
  The technique consists of two parts:  "conversation-driven specification
generation", then "mutation-based specification generation".  The novelty lies
in the fix guidance and in mutating an incorrect specification to make it
correct.
  "conversation-driven specification generation" is not conversational in the
natural language sense.  It iterates with an LLM to produce a specification.  On
each iteration, ask the LLM for (an improved) specification; the input is the
code, the previous specification, the errors produced by the verifier for the
previous iteration, and guidance about how to fix the errors.  (This is rather
obvious -- It is used by RustAssistant [DeligiannisLMPR2025], for example --
except for the guidance.)  Terminate when the specification verifies
or after 10 rounds.  The prompt is few-shot:  it contains 4 randomly-chosen
examples from their dataset, together with their specifications.  I wonder how
similar different specifications are -- that is, how much the few-shot examples
help the LLM for this particular dataset.  For 4 of the executions (on
expectation), one of randomly-chosen examples will be exactly the program being
analyzed.
  For iterations after the first, the prompt gives guidance about correcting the
problem.  "through a massive amount of experiments, we summarize several types
of common verification failures reported by the verifier. For each kind of
error, we provide guidance in the natural language to facilitate the model in
generating correct specifications."  Does this imply that their technique is
difficult to apply to new situations?  They don't report what the guidance was.
They didn't evaluate the effect of the guidance (say, by an ablation study).
They don't report trying to do the correction symbolically.
  "Mutation-based specification generation" is really mutation-based
specification *correction*.  Even if the result of the previous step is not
correct, it may be close to correct.  Apply mutation operators to the
specification to see whether some variant of the specification verifies.  There
is some heuristic about the order in which to apply the mutations and
combinations of them.  This part does not use an LLM.  The 15 mutation operators
seem tuned to array invariants, which I suspect is the majority of their
invariants.  This correction is responsible for about 1/4 of SpecGen's overall
success.  AutoGen outperforms the "conversational specification generation";
if it were combined with the mutation-based correction, would that outperform
SpecGen?
  The paper gives only one example of output, in figure 1.
  I could not understand the explanation in section 3.C of the heuristics that
choose among mutated specifications.  Algorithm 1 contains no comments, and
there is such as "a subset E_selected of mutated specifications that can pass
the verification. The selected subset E_selected is initialized with E_t.", but
E_t consists of specifications that do *not* pass verification.  I think it may
work like this:  add up the cost of a set of mutations, and prioritize the
sets with the lowest cost.
  The evaluation is on a dataset of programs all of which verify perfectly with
OpenJML.  Any program that does not verify is dropped from their dataset and
does not appear in their experimental numbers.  The programs are selected from
the SV-COMP dataset (89\% of which are loop-free), from Nilizadeh et al. [43],
and from LeetCode.
  The "Prob." column in Table 2 and Table 6, is the "average success
probability" over 10 trials.  That is not as interesting as whether the
technique ever succeeds -- that is, prior work AutoSpec is just as good as their
SpecGen for the "Sequential" column, but the table only highlights their
SpecGen.
  It is no surprise that Daikon does poorly in their evaluation, since they
didn't subset their data to only programs whose specifications are within
Daikon's grammar.  Nor did they expand Daikon's grammar to include properties
that are necessary.  Nor do they report using Daikon to compute loop invariants
(which are necessary for verifying many of their programs).  The evaluation
isn't whether Daikon's output is correct or useful, only whether it forms a full
specification that OpenJML can fully verify (which is not a task Daikon is tuned
for).
  The paper is full of unfounded claims.  "The selected programs are highly
representative": no evidence is given.  "specifications that correctly and
comprehensively describe the semantics of complex programs": no evaluation of
the comprehensiveness is given.  We know that the generated specifications are
verifiable, but they might be quite weak and still verify.  Though OpenJML might
require that properties such as nullness and array indexing are satisfied.
  The authors did a user study on scoring how good the specifications were.
However, they did not include previous work, such as AutoSpec.  The raters were
the authors' graduate students, apparently.
  The artifact is at https://github.com/Lezhi-Ma/SpecGen-Artifact .


@Book{PorterZ2024,
  author =    {Leo Porter and Daniel Zingaro},
  title =        {Learn AI-Assisted Python Programming, Second Edition: With GitHub Copilot and ChatGPT},
  publisher =    {Manning},
  year =         2024,
}



@article{10.1145/3715003,
author = {Terragni, Valerio and Vella, Annie and Roop, Partha and Blincoe, Kelly},
title = {The Future of AI-Driven Software Engineering},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715003},
doi = {10.1145/3715003},
abstract = {A paradigm shift is underway in Software Engineering, with AI systems such as LLMs playing an increasingly important role in boosting software development productivity. This trend is anticipated to persist. In the next years, we expect a growing symbiotic partnership between human software developers and AI. The Software Engineering research community cannot afford to overlook this trend; we must address the key research challenges posed by the integration of AI into the software development process. In this article, we present our vision of the future of software development in an AI-driven world and explore the key challenges that our research community should address to realize this vision.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {120},
numpages = {20},
keywords = {Software Engineering, Artificial Intelligence, Machine Learning, Large Language Models, APIs, Libraries, Software Testing, Requirements Engineering}
}

@inproceedings{10.1145/3597503.3639226,
author = {Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh},
title = {Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639226},
doi = {10.1145/3597503.3639226},
abstract = {Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1\% to 47.3\% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5\% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {82},
numpages = {13},
keywords = {code translation, bug taxonomy, llm},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@article{10.1145/3715007,
author = {Chen, Xiang and Gao, Chaoyang and Chen, Chunyang and Zhang, Guangbei and Liu, Yong},
title = {An Empirical Study on Challenges for LLM Application Developers},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715007},
doi = {10.1145/3715007},
abstract = {In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI's ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development.Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {205},
numpages = {37},
keywords = {Mining Software Repository, Empirical Study, LLM Developer, Development Challenges, Prompt Engineering}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Evaluation benchmarks
%%%



@inproceedings{10.5555/3618408.3619164,
author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
title = {DS-1000: a natural and reliable benchmark for data science code generation},
year = 2023,
publisher = {JMLR.org},
booktitle = {ICML: Proceedings of the 40th International Conference on Machine Learning},
articleno = 756,
numpages = 27,
location = {Honolulu, Hawaii, USA},
abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from Stack-Overflow. Second, our automatic evaluation is highly specific (reliable) - across all Codex-002- predicted solutions that our evaluation accepts, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3\% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
}




@INPROCEEDINGS {10174231,
author = { Tony, Catherine and Mutas, Markus and Ferreyra, Nicolas E. Diaz and Scandariato, Riccardo },
booktitle = { 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR) },
title = {{ LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations }},
year = {2023},
volume = {},
ISSN = {},
pages = {588-592},
abstract = { Large Language Models (LLMs) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language (NL) descriptions by learning languages and programming practices from public GitHub repositories. Although LLMs promise an effortless NL-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present LLMSecEval, a dataset containing 150 NL prompts that can be leveraged for assessing the security performance of such models. Such prompts are NL descriptions of code snippets prone to various security vulnerabilities listed in MITRE’s Top 25 Common Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by LLMs. As a practical application, we show how LLMSecEval can be used for evaluating the security of snippets automatically generated from NL descriptions. },
keywords = {Computer languages;Codes;Natural languages;Programming;Software;Security;Data mining},
doi = {10.1109/MSR59073.2023.00084},
url = {https://doi.ieeecomputersociety.org/10.1109/MSR59073.2023.00084},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =May}
The prompts are generated by Codex from CWE-related code snippets, then
"curated" (filtered? edited?) by the authors.
There are no test cases; the authors did an experiment where they ran their CodeQL
tool on the generated code:  "We used built-in QL queries to detect 18 of the
Top 25 CWEs in code created using LLMSecEval."










@article{Chen2021EvaluatingLL,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Pond{\'e} and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mo Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and David W. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and Igor Babuschkin and Suchir Balaji and Shantanu Jain and Andrew Carr and Jan Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew M. Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.03374},
  OMITurl={https://api.semanticscholar.org/CorpusID:235755472}
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Education
%%%

%%% Intro CS classes

@inproceedings{10.1145/3754508.3754517,
author = {Aruleba, Kehinde and Oyelere, Solomon Sunday and Obaido, George Rabeshi and Sanusi, Ismaila Temitayo},
title = {A Scoping Review of Student and Educator Engagement with Large Language Models in Introductory Programming Education},
year = {2025},
isbn = {9798400720789},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3754508.3754517},
doi = {10.1145/3754508.3754517},
abstract = {As Large Language Models (LLMs) like ChatGPT and GitHub Copilot gain traction in computing education, understanding their role in introductory programming (CS1) is essential. This scoping review synthesises 38 empirical studies published between 2022 and 2024, focusing on student and educator engagement with LLMs in CS1 contexts. Following Arksey and O’Malley’s five-stage framework and PRISMA-ScR guidelines, we identify four thematic areas: (1) varied student prompting behaviours, from surface-level code copying to iterative refinement; (2) evolving educator practices, from passive allowance to guided integration; (3) assessment-related tensions, notably the “assistance dilemma”; and (4) ethical concerns around bias, integrity, and access. While LLMs support debugging and code comprehension, their value depends on pedagogical framing and learner agency. Gaps remain in longitudinal research, diverse learner representation, and alignment with curriculum frameworks. We offer practical recommendations for scaffolded GenAI integration, prompt engineering strategies, and ethical classroom use. This review supports the development of CS1 curricula that foster critical AI literacy, inclusive participation, and thoughtful engagement with human–AI collaboration in programming education.},
booktitle = {UKICER: Proceedings of the 2025 Conference on UK and Ireland Computing Education Research},
articleno = {12},
numpages = {8},
keywords = {Large Language Models (LLMs), Programming Education, Introductory Computer Science (CS1), Generative AI, Student Engagement},
}


@inproceedings{10.1145/3641555.3705201,
author = {Bejarano, Andres and Dickey, Ethan and Setsma, Rhianna},
title = {Implementing the AI-Lab Framework: Enhancing Introductory Programming Education for CS Majors},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705201},
doi = {10.1145/3641555.3705201},
abstract = {The advent of generative AI tools presents novel opportunities and challenges in computer science education, particularly in introductory programming courses. This study explores the implementation of AI-Lab, a framework designed to guide students in the effective and ethical use of generative AI, in this case ChatGPT, in academic settings without compromising skill development. Conducted during Spring 2024, our use of the intervention targeted over 500 Computer Science and Data Science majors enrolled in their major-specific Data Structures and Algorithms courses. The AI-Lab framework enabled students to develop both conceptual questions and c++ and Python programs by interacting with ChatGPT and iteratively correcting its errors. Focus groups and post-intervention surveys revealed a generally positive experience. Students appreciated the ability to leverage AI for tasks outside their major, recognizing the value of understanding correct solutions through AI-assisted programming. Moreover, the guided use of generative AI by professors alleviated concerns regarding academic dishonesty, fostering a supportive learning environment. Despite these benefits, students expressed awareness of the potential drawbacks of over-reliance on AI, noting the risk of impeding their professional growth. Nevertheless, they acknowledged the practical utility of AI for non-major related tasks. This study highlights the importance of incorporating structured AI training in curricula to balance skill development and ethical AI usage, offering insights for broader applications in higher education.},
booktitle = {SIGCSETS: Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1383–1384},
numpages = {2},
keywords = {ai lab, ai-assisted programming, ai-lab framework, ethical ai usage, generative ai in education, skill development with ai, structured ai training},
location = {Pittsburgh, PA, USA},
}


@inproceedings{10.1145/3723178.3723268,
author = {Sadat Shanto, Shakib and Ahmed, Zishan and Jony, Akinul Islam},
title = {Generative AI for Programming Education: Can ChatGPT Facilitate the Acquisition of Fundamental Programming Skills for Novices?},
year = {2025},
isbn = {9798400713828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723178.3723268},
doi = {10.1145/3723178.3723268},
abstract = {Modern Generative AI (GAI) systems like ChatGPT have sparked much interest in their potential to revolutionize programming education, especially for beginners. However, the existing empirical data regarding the effectiveness of technologies like ChatGPT as autonomous programming tutors is presently limited. The present study investigates the capacity of ChatGPT to facilitate the acquisition of fundamental programming skills for novice programmers without human assistance. This study puts forth a conceptual framework (APEC - Adaptive Programming Education via ChatGPT) that integrates both bottom-up and top-down approaches, incorporating ChatGPT as the principal instructor for the study of programming. An empirical study was undertaken to assess the usefulness of ChatGPT as a tool for teaching novice programmers a new programming language. This empirical study was conducted on 20 undergraduate students. To provide an expert assessment of the quality of the responses, a survey was conducted with three programming experts proficient in Python. The survey findings indicate that ChatGPT is proficient in explaining core principles such as variables, data types, and control statements through conversational exchanges, adopting an intelligent and logical methodology. Nevertheless, certain constraints arise when dealing with increasingly complex topics.},
booktitle = {ICCA: Proceedings of the 3rd International Conference on Computing Advancements},
pages = {685–692},
numpages = {8},
keywords = {Generative AI, ChatGPT, Programming Education, Educational Technology, Higher Education},
}


%%% Non-intro CS classes

@inproceedings{10.1145/3626252.3630927,
author = {Kirova, Vassilka D. and Ku, Cyril S. and Laracy, Joseph R. and Marlowe, Thomas J.},
title = {Software Engineering Education Must Adapt and Evolve for an LLM Environment},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630927},
doi = {10.1145/3626252.3630927},
abstract = {In the era of artificial intelligence (AI), generative AI, and Large Language Models (LLMs) in particular, have become increasingly significant in various sectors. LLMs such as GPT expand their applications, from content creation to advanced code completion. They offer unmatched opportunities but pose unique challenges to the software engineering domain. This paper discusses the necessity and urgency for software engineering education to adapt and evolve to prepare software engineers for the emerging LLM environment. While existing literature and social media have investigated AI's integration into various educational spheres, there is a conspicuous gap in examining the specifics of LLMs' implications for software engineering education. We explore the goals of software engineering education, and changes to software engineering, software engineering education, course pedagogy, and ethics. We argue that a holistic approach is needed, combining technical skills, ethical awareness, and adaptable learning strategies. This paper seeks to contribute to the ongoing conversation about the future of software engineering education, emphasizing the importance of adapting and evolving to remain in sync with rapid advancements in AI and LLMs. It is hoped that this exploration will provide valuable insights for educators, curriculum developers, and policymakers in software engineering.},
booktitle = {SIGCSE: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {666–672},
numpages = {7},
keywords = {chatgpt, generative ai, large language models (llms), responsible ai, software engineering, software engineering education, software engineering ethics, software ethics},
location = {Portland, OR, USA},
}

@inproceedings{10.1145/3702653.3744328,
author = {Roy, Nimisha and Horielko, Oleksandr and Omojokun, Olufisayo},
title = {Benchmarking of Generative AI Tools in Software Engineering Education: Formative Insights for Curriculum Integration},
year = {2025},
isbn = {9798400713415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702653.3744328},
doi = {10.1145/3702653.3744328},
abstract = {Generative Artificial Intelligence (Gen-AI) has revolutionized software engineering (SE) by automating tasks across design, coding, and testing [1] [2]. Tools like ChatGPT and GitHub Copilot streamline code generation, architectural modeling, debugging, and test-case creation [3] [4]. Despite their rapid adoption in industry, the pedagogical implications of these tools in computing education have not been systematically examined. This study solves the existing gap by conducting a comprehensive benchmarking study of Gen-AI tools across four core SE phases— design documentation, feature implementation, debugging support, and testing — to address two research questions:RQ1: What strengths and limitations do Gen-AI tools exhibit in each phase?RQ2: How can insights from benchmarking inform effective integration of Gen-AI into SE curricula?To answer these questions, a diverse set of Gen-AI tools is evaluated, ranging from design-focused assistants such as Lucidchart, Mermaid.js and UIzard; implementation-oriented systems including GitHub Copilot, TabNine, Codeium and Supermaven; debugging supports like GPT-4 and Claude 3.5 Sonnet; and testing frameworks such as Testim, Mabl and Applitools—while also surveying emerging platforms (as of summer 2024) like Replit, Postman, Visily, Gemini, Eraser.io and others. For each tool and development phase, we applied phase-specific metrics: in design documentation, we assessed diagram accuracy, completeness, user effort, and IDE integration; in feature implementation, we measured pattern-based code generation quality, code-completion effectiveness, refactoring robustness, and UI/UX scaffolding; in debugging, we evaluated error-detection accuracy, hallucination rates, and clarity of explanatory feedback; and in testing, we examined test-case relevance and defect-detection coverage. Across all phases, we tracked prompt engineering complexity as a key mediating factor influencing tool performance.Our evaluation reveals speed-fidelity trade-offs: Code-completion assistants accelerate boilerplate generation but demand manual oversight to ensure cross-file consistency and manage higher-order abstractions; diagramming tools can produce precise UML models with minimal effort— but at the cost of iterative prompt refinement for complex cases; LLM debuggers deliver context-sensitive fixes yet suffer from nontrivial hallucination rates; testing generators exhibit wide variance in edge-case coverage. On average, tools needed 2.4 prompt iterations for usable diagrams and 1.5 prompts for bug fixes, underscoring the human effort in guiding AI.We recommend a scaffolded framework for integrating Gen-AI into SE education by: embedding AI tools into hands-on assignments, to explore tasks in a controlled context; by structuring small team projects in which one subgroup uses AI assistants while the other completes the same tasks manually (covering design, implementation, debugging and testing) to surface contrasts in workflow, tool strengths, and human reasoning; by requiring students to maintain a reflective journal documenting their AI usage and prompt-engineering strategies, fostering metacognitive insight into how tool inputs shape outputs; and by equipping learners with decision making criteria, teaching them to evaluate AI assistants according to task fit- preparing them to leverage AI responsibly across SE phases in its evolving landscape.},
booktitle = {ICER: Proceedings of the 2025 ACM Conference on International Computing Education Research V.2},
pages = {3},
numpages = {1},
keywords = {Generative AI, Software Engineering Education, Benchmarking, Prompt Engineering, Hallucination, Productivity},
}

@inproceedings{10.1145/3639474.3640061,
author = {Frankford, Eduard and Sauerwein, Clemens and Bassner, Patrick and Krusche, Stephan and Breu, Ruth},
title = {AI-Tutoring in Software Engineering Education},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640061},
doi = {10.1145/3639474.3640061},
abstract = {With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.},
booktitle = {ICSE-SEET: Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {309–319},
numpages = {11},
keywords = {programming education, automated programming assessment systems, artificial intelligence, ChatGPT, OpenAI, ChatBots},
location = {Lisbon, Portugal},
}


@inproceedings{10.1145/3633053.3633057,
author = {Petrovska, Olga and Clift, Lee and Moller, Faron and Pearsall, Rebecca},
title = {Incorporating Generative AI into Software Development Education},
year = {2024},
isbn = {9798400709326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633053.3633057},
doi = {10.1145/3633053.3633057},
abstract = {This paper explores how Generative AI can be incorporated into software development education. We present examples of formative and summative assessments, which explore various aspects of ChatGPT, including its coding capabilities, its ability to construct arguments as well as ethical issues of using ChatGPT and similar tools in education and the workplace. Our work is inspired by the insights from surveys that show that the learners on our Degree Apprenticeship Programme have a great interest in learning about and exploiting emerging AI technology. Similarly, our industrial partners have a clear interest for their employees to be formally prepared to use GenAI in their software engineering roles. In this vein, it is proposed that embedding the use of GenAI tools in a careful and creative way - by developing assessments which encourage learners to critically evaluate AI output - can be beneficial in helping learners understand the subject material being taught without the risk of the AI tools “doing the homework”.},
booktitle = {CEP: Proceedings of the 8th Conference on Computing Education Practice},
pages = {37–40},
numpages = {4},
keywords = {apprenticeship, assessment, education, generative AI, software engineering},
location = {Durham, United Kingdom},
}


%%% Other CS education

@inproceedings{10.1145/3719487.3719519,
author = {Chang, Chi-In and Choi, Wan-Chong and Choi, Iek-Chong and Lei, Huey},
title = {A Systematic Literature Review of the Practical Applications of Artificial Intelligence-Generated Content (AIGC) Using OpenAI ChatGPT, Copilot, and Codex in Programming Education},
year = {2025},
isbn = {9798400717413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719487.3719519},
doi = {10.1145/3719487.3719519},
abstract = {This systematic literature review investigated the practical applications of Artificial Intelligence-Generated Content (AIGC) tools, specifically ChatGPT, Copilot, and Codex, in programming education. The review synthesized current research trends and key applications, with a particular focus on large language models (LLMs) and advanced chatbots. The findings revealed a significant increase in research interest following the release of ChatGPT, with most studies concentrating on university-level programming education. The review identified eight primary applications of AIGC tools in programming education, including evaluating AI performance in solving programming tasks, providing AI-driven code generation and assistance, automating assessment review and feedback, delivering personalized learning and tutoring, supporting educators and instructional design, investigating educator and student perceptions, checking for plagiarism, and exploring AI's impact on curriculum design. Despite the promising potential of these tools, critical gaps remained, particularly in primary and secondary education, as well as in online and blended learning environments. Future research was recommended to address these areas and to explore a broader range of AIGC tools to unlock their full potential for enhancing programming education.},
booktitle = {ICEEL: Proceeding of the 2024 8th International Conference on Education and E-Learning},
pages = {13–19},
numpages = {7},
keywords = {Applications of AIGC, Artificial intelligence generated content, ChatGPT, Chatbots, Educational technology, Large language models, Programming education, Systematic literature review},
}

@inproceedings{10.1145/3719384.3719439,
author = {Chang, Chi In and Choi, Wan Chong and Choi, Iek Chong},
title = {Challenges and Limitations of Using Artificial Intelligence Generated Content (AIGC) with ChatGPT in Programming Curriculum: A Systematic Literature Review},
year = {2025},
isbn = {9798400717925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719384.3719439},
doi = {10.1145/3719384.3719439},
abstract = {This systematic literature review examined the challenges and limitations of integrating Artificial Intelligence Generated Content (AIGC) tools into programming curricula. Following Kitchenham's framework, a comprehensive search was conducted across ACM Digital Library and Web of Science databases to select 22 relevant peer-reviewed papers. Key issues identified included increased plagiarism risk due to AI-generated unique code, accuracy and reliability concerns of AI outputs, privacy and data security risks, curriculum changes to incorporate prompt engineering and critical thinking, and challenges in traditional assessment methods. Additionally, the review highlighted the potential over-reliance on AI tools, which could hinder the development of fundamental programming skills. Implementation challenges and inadequate training for educators further complicate the integration process. Addressing these challenges is crucial for leveraging the benefits of AIGC tools while maintaining academic integrity and enhancing learning outcomes in programming education. Future research should focus on developing strategies to mitigate these issues and ensure the responsible use of AI in education.},
booktitle = {AICCC: Proceedings of the 2024 7th Artificial Intelligence and Cloud Computing Conference},
pages = {372–378},
numpages = {7},
keywords = {Artificial intelligence generated content, Challenges of AIGC, ChatGPT, Large language models, Programming curriculum, Systematic literature review},
}



@inproceedings{10.1145/3696630.3727247,
author = {S\"{o}lch, Maximilian and Dietrich, Felix T. J. and Krusche, Stephan},
title = {Direct Automated Feedback Delivery for Student Submissions based on LLMs},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3727247},
doi = {10.1145/3696630.3727247},
abstract = {Timely and individualized feedback is essential for students' learning progress and motivation, yet providing such feedback has become increasingly challenging due to growing student numbers. This has resulted in a time-consuming, repetitive, and often manual task for educators, contributing to a high workload.This paper presents DAFeeD, an LLM-based approach for automated feedback on student submissions across various exercise domains. The defined feedback process enables interactive learning by allowing students to submit solutions multiple times and automatically receive iterative LLM feedback on their submission attempts before deadlines. By incorporating task details, grading criteria, student solutions, and custom instructions into the prompt, DAFeeD provides clear, personalized, and pedagogically meaningful feedback to support continuous improvement.To evaluate the feedback process, we implemented DAFeeD in an open-source reference implementation integrated into the learning platform Artemis. A controlled study with students working on a programming task in a supervised environment showed that students found the feedback relevant and beneficial. They reported feeling more comfortable and willing to request automated feedback due to its convenience and immediacy. Additionally, deploying DAFeeD in a software engineering course with 450 students demonstrated improvements in student performance and encouraged iterative refinement through multiple submissions.These findings highlight DAFeeD's potential to enhance feedback processes in computing education, improving both learning efficiency and student outcomes.},
booktitle = {FSE Companion: Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {901–911},
numpages = {11},
keywords = {software engineering, education, grading, formative feedback},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
}



%%% Other

@inproceedings{10.1145/3637528.3671498,
author = {Wen, Qingsong and Liang, Jing and Sierra, Carles and Luckin, Rose and Tong, Richard and Liu, Zitao and Cui, Peng and Tang, Jiliang},
title = {AI for Education (AI4EDU): Advancing Personalized Education with LLM and Adaptive Learning},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671498},
doi = {10.1145/3637528.3671498},
abstract = {Recent advanced AI technologies, especially large language models (LLMs) like GPTs, have significantly advanced the field of data mining and led to the development of various LLM-based applications. AI for education (AI4EDU) is a vibrant multi-disciplinary field of data mining, machine learning, and education, with increasing importance and extraordinary potential. In this field, LLM and adaptive learning-based models can be utilized as interfaces in human-in-the-loop education systems, where the model serves as a mediator among the teacher, students, and machine capabilities, including its own. This perspective has several benefits, including the ability to personalize interactions, allow unprecedented flexibility and adaptivity for human-AI collaboration and improve the user experience. However, several challenges still exist, including the need for more robust and efficient algorithms, designing effective user interfaces, and ensuring ethical considerations are addressed. This workshop aims to bring together researchers and practitioners from academia and industry to explore cutting-edge AI technologies for personalized education, especially the potential of LLMs and adaptive learning technologies.},
booktitle = {KDD: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6743–6744},
numpages = {2},
keywords = {adaptive learning, edtech, education, llm},
location = {Barcelona, Spain},
}


@inproceedings{10.1145/3641554.3701863,
author = {Raihan, Nishat and Siddiq, Mohammed Latif and Santos, Joanna C.S. and Zampieri, Marcos},
title = {Large Language Models in Computer Science Education: A Systematic Literature Review},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701863},
doi = {10.1145/3641554.3701863},
abstract = {Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding. Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks. Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications. Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code. We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education. We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development. We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.},
booktitle = {SIGCSETS: Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {938–944},
numpages = {7},
keywords = {code generation, cs education, large language models},
location = {Pittsburgh, PA, USA},
}


@inproceedings{10.1145/3706599.3719857,
author = {Wang, Xinyu Jessica and Lee, Christine P. and Mutlu, Bilge},
title = {LearnMate: Enhancing Online Education with LLM-Powered Personalized Learning Plans and Support},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719857},
doi = {10.1145/3706599.3719857},
abstract = {With the increasing prevalence of online learning, adapting education to diverse learner needs remains a persistent challenge. Recent advancements in artificial intelligence (AI), particularly large language models (LLMs), promise powerful tools and capabilities to enhance personalized learning in online educational environments. In this work, we explore how LLMs can improve personalized learning experiences by catering to individual user needs toward enhancing the overall quality of online education. We designed personalization guidelines based on the growing literature on personalized learning to ground LLMs in generating tailored learning plans. To operationalize these guidelines, we implemented LearnMate, an LLM-based system that generates personalized learning plans and provides users with real-time learning support. We discuss the implications and future directions of this work, aiming to move beyond the traditional one-size-fits-all approach by integrating LLM-based personalized support into online learning environments.},
booktitle = {CHI EA: Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {373},
numpages = {10},
keywords = {large-language models; personalized learning; human-centered AI},
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Other
%%%

@InProceedings{TanYKZ2007,
  author = 	 "Tan, Lin and Yuan, Ding and Krishna, Gopal and Zhou, Yuanyuan",
  title = 	 "/*{iComment}: Bugs or Bad Comments?*/",
  crossref =     "SOSP2007",
  pages = 	 "145--158",
}


@InProceedings{TanZP2011,
  author = 	 "Tan, Lin and Zhou, Yuanyuan and Padioleau, Yoann",
  title = 	 "{aComment}: Mining annotations from comments and code to detect interrupt related concurrency bugs",
  crossref =  "ICSE2011",
  pages = 	 "11--20",
}






@InProceedings{YeSMBL2016,
  author =       "Xin Ye and Hui Shen and Xiao Ma and Razvan Bunescu and Chang Liu",
  title =        "From word embeddings to document similarities for improved information retrieval in software engineering",
  crossref =     "ICSE2016",
  NEEDpages =     "*",
}
































@InProceedings{AllamanisBBS2014,
  author = 	 "Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles",
  title = 	 "Learning natural coding conventions",
  crossref =     "FSE2014",
  pages = 	 "281--293",
}


@InProceedings{PanditaXZXOP2012,
  author = 	 "Pandita, Rahul and Xiao, Xusheng and Zhong, Hao and Xie, Tao and Oney, Stephen and Paradkar, Amit",
  title = 	 "Inferring method specifications from natural language {API} descriptions",
  crossref =     "ICSE2012",
  pages = 	 "815--825",
}












@InProceedings{HindleBSGD2012,
  author = 	 "Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar",
  title = 	 "On the Naturalness of Software",
  crossref =     "ICSE2012",
  pages = 	 "837--847",
}




@InProceedings{HowardGPVS2013,
  author = 	 "Howard, Matthew J. and Gupta, Samir and Pollock, Lori and Vijay-Shanker, K.",
  title = 	 "Automatically mining software-based, semantically-similar words from comment-code mappings",
  crossref =     "MSR2013",
  pages = 	 "377--386",
}


@InProceedings{GuptaMPVS2013,
  author = 	 "Samir Gupta and Sana Malik and Lori Pollock and K. Vijay-Shanker",
  title = 	 "Part-of-speech tagging of program identifiers for improved text-based software engineering tools",
  crossref =     "ICPC2013",
  pages = 	 "3--12",
}


@InProceedings{SridharaHMPVS2010,
  author = 	 "Sridhara, Giriprasad and Hill, Emily and Muppaneni, Divya and Pollock, Lori and Vijay-Shanker, K.",
  title = 	 "Towards automatically generating summary comments for {Java} methods",
  crossref =     "ASE2010",
  pages = 	 "43--52",
}


@InProceedings{HillFBSNPV2008,
  author = 	 "Hill, Emily and Fry, Zachary P. and Boyd, Haley and Sridhara, Giriprasad and Novikova, Yana and Pollock, Lori and Vijay-Shanker, K.",
  title = 	 "{AMAP}: Automatically mining abbreviation expansions in programs to enhance software maintenance tools",
  crossref =     "MSR2008",
  pages = 	 "79--88",
}















































@InProceedings{ArnaoudovaEOGA2010,
  author = 	 "Arnaoudova, Venera and Eshkevari, Laleh and Oliveto, Rocco and Gueheneuc, Yann-Gael and Antoniol, Giuliano",
  title = 	 "Physical and conceptual identifier dispersion: Measures and relation to fault proneness",
  crossref =     "ICSM2010",
  pages = 	 "1--5",
}




@Article{LawrieMFB2007,
  author = 	 "Lawrie, Dawn and Morrell, Christopher and Feild, Henry and Binkley, David",
  title = 	 "Effective identifier names for comprehension and memory",
  journal = 	 "Innovations in Systems and Software Engineering",
  year = 	 2007,
  volume = 	 3,
  number = 	 4,
  pages = 	 "303--318",
  month = 	 dec,
  abstract =
   "Readers of programs have two main sources of domain information:
    identifier names and comments. When functions are uncommented, as many are,
    comprehension is almost exclusively dependent on the identifier
    names. Assuming that writers of programs want to create quality identifiers
    (e.g., identifiers that include relevant domain knowledge), one must ask
    how should they go about it. For example, do the initials of a concept name
    provide enough information to represent the concept? If not, and a longer
    identifier is needed, is an abbreviation satisfactory or does the concept
    need to be captured in an identifier that includes full words? What is the
    effect of longer identifiers on limited short term memory capacity? Results
    from a study designed to investigate these questions are reported. The
    study involved over 100 programmers who were asked to describe 12 different
    functions and then recall identifiers that appeared in each function. The
    functions used three different levels of identifiers: single letters,
    abbreviations, and full words. Responses allow the extent of comprehension
    associated with the different levels to be studied along with their impact
    on memory. The functions used in the study include standard computer
    science textbook algorithms and functions extracted from production
    code. The results show that full-word identifiers lead to the best
    comprehension; however, in many cases, there is no statistical difference
    between using full words and abbreviations. When considered in the light of
    limited human short-term memory, well-chosen abbreviations may be
    preferable in some situations since identifiers with fewer syllables are
    easier to remember.",
}


@Article{DeissenboeckP2006,
  author = 	 "Deissenboeck, Florian and Pizka, Markus",
  title = 	 "Concise and consistent naming",
  journal = 	 "Software Quality Journal",
  year = 	 2006,
  volume = 	 14,
  number = 	 3,
  pages = 	 "261--282",
  month = 	 sep,
}


@InProceedings{MihalceaCS2006,
  author = 	 "Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo",
  title = 	 "Corpus-based and knowledge-based measures of text semantic similarity",
  crossref =     "AAAI2006",
  pages = 	 "775--780",
}


@InProceedings{LawrieMB2010,
  author = 	 "Dawn Lawrie and Christopher Morrell and Dave Binkley",
  title = 	 "Normalizing source code vocabulary",
  crossref =     "WCRE2010",
  pages = 	 "3-12",
}




@InProceedings{MotwaniBrun2019,
  author = 	 "Motwani, Manish and Brun, Yuriy",
  title = 	 "Automatically Generating Precise Oracles from Structured Natural Language Specifications",
  crossref =  "ICSE2019",
  pages = 	 "188--199",
}


@InProceedings{HuLXLJ2018,
  author = 	 "Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi",
  title = 	 "Deep code comment generation",
  crossref =  "ICPC2018",
  pages = 	 "200--210",
}


@Misc{LouisDBS2018,
  author = 	 "Annie Louis and Santanu Kumar Dash and Earl T. Barr and Charles Sutton",
  title = 	 "Deep Learning to Detect Redundant Method Comments",
  howpublished = "\url{http://arxiv.org/abs/1806.04616}",
  month = 	 jun,
  year = 	 2018,
}


@InProceedings{MikolovSCCD2013,
  author = 	 "Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey",
  title = 	 "Distributed representations of words and phrases and their compositionality",
  crossref =  "NIPS2013",
  pages = 	 "3111--3119",
}

@InProceedings{MovshovitzAttiasC2013,
  author = 	 "Movshovitz-Attias, Dana and Cohen, William W.",
  title = 	 "Natural language models for predicting programming comments",
  crossref =  "ACL2013short",
  pages = 	 "35--40",
}

@InProceedings{BuzeW2010,
  author = 	 "Buse, Raymond P.L. and Weimer, Westley R.",
  title = 	 "Automatically documenting program changes",
  crossref =  "ASE2010",
  pages = 	 "33-42",
}

@InProceedings{PascarellaB2017,
  author = 	 "Pascarella, Luca and Bacchelli, Alberto",
  title = 	 "Classifying code comments in {Java} open-source software systems",
  crossref =  "MSR2017",
  pages = 	 "227-237",
  supersededby = "PascarellaBB2019"
}

@Article{PascarellaBB2019,
  author = 	 "Pascarella, Luca and Bruntink, Magiel and Bacchelli, Alberto",
  title = 	 "Classifying code comments in {Java} software systems",
  journal = 	 JEmpiricalSE,
  year = 	 2019,
  volume = 	 24,
  number = 	 3,
  pages = 	 "1499-1537",
  month = 	 jun,
}



@InProceedings{DevlinCLT2019,
  author = 	 "Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova",
  title = 	 "{BERT}: Pre-training of deep bidirectional transformers for language understanding",
  crossref =  "NAACL-HLT2019",
  pages = 	 "4171--4186",
}


@InProceedings{HossainD2025,
  author =       {Hossain, Soneya Binta and Dwyer, Matthew},
  title =        {{TOGLL}: Correct and Strong Test Oracle Generation with {LLMs}},
  crossref =  {ICSE2025},
  abstract =  {Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have shown impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation. In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on a large dataset consisting of 110 Java projects. Utilizing the most effective fine-tuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 unseen large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles than TOGA. Regarding bug detection effectiveness, TOGLL can detect 1,023 unique mutants that EvoSuite cannot, which is ten times more than what TOGA can detect. Additionally, TOGLL significantly outperforms TOGA in detecting real bugs from the Defects4J dataset.},
}


@INPROCEEDINGS{10189162,
  author={Siddiq, Mohammed Latif and Samee, Abdus and Azgor, Sk Ruhul and Haider, Md. Asif and Sawraz, Shehabul Islam and Santos, Joanna C. S.},
  booktitle={2023 IEEE/ACM 2nd International Workshop on Natural Language-Based Software Engineering (NLBSE)},
  title={Zero-shot Prompting for Code Complexity Prediction Using GitHub Copilot},
  year={2023},
  volume={},
  number={},
  pages={56-59},
  abstract={Code generation models are gaining popularity because they can produce correct code from a prompt, speeding up the software development process. GitHub Copilot is currently one of the most commonly used tools for code generation. This tool is based on GPT3, a Large Language Model (LLM), and can perform zero-shot prompting tasks i.e., tasks for which the model is not specifically trained. In this paper, we describe a preliminary study that investigates whether GitHub Copilot can predict the runtime complexity of a given program using zero- shot prompting. In our study, we found that GitHub Copilot can correctly predict the runtime complexity 45.44% times in the first suggestion and 56.38 % times considering all suggestions. We also compared Copilot to other machine learning, neural network, and transformer-based approaches for code complexity prediction. We observed that Copilot outperformed other approaches for predicting code with linear complexity $\mathbf{O}(n)$.},
  keywords={Codes;Runtime;Computational modeling;Predictive models;Transformers;Software;Complexity theory;code generation;computational complexity;trans-former;zero-shot prompting;pre-trained model;GitHub copilot},
  doi={10.1109/NLBSE59153.2023.00018},
  ISSN={},
  month=May,}
GitHub Copilot can correctly predict the runtime complexity 45.44\% times in the
first suggestion (top-1) and 56.38 \% times considering all suggestions (top-all).
Short 4-page paper.  They don't come out and say that one should re-prompt.










%  LocalWords:  InProceedings TanYKZ2007 Gopal Zhou Yuanyuan iComment Iyer pdf
%  LocalWords:  booktitle SOSP2007 SOSP2007date SOSP2007addr Benwen NN pre url
%  LocalWords:  testEntrySetClearChangesMap Srinivasan Ioannis Konstas Xin
%  LocalWords:  testSettingHeightThatIsTooSmallLeavesHeightUnchanged LSTM
%  LocalWords:  Zettlemoyer YeSMBL2016 Shen Xiao Razvan Bunescu Liu MRR Za
%  LocalWords:  ICSE2016 NEEDpages ICSE2016date ICSE2016addr Wiki LSA CCG
%  LocalWords:  stemmer Kushman Barzilay Turkers regex regexes Mise Kiddon
%  LocalWords:  Ganesa Thandavam Ponnuraj Yejin Choi Branavan Miltiadis xj
%  LocalWords:  Allamanis AAAI Briand Briand's Hirschberg uncompelling xk
%  LocalWords:  Movshovitz Attias ICPC preprocess pickaxe xl Convolutional
%  LocalWords:  Hao Peng ie camelcase tokenizer Naturalize's NLC2CMD OpenAI
% LocalWords:  Agarwal Mayank Chakraborti Tathagata Fu Quchen Gros
% LocalWords:  Maene Jaron Talamadupula Kartik Teng Zhongwei
